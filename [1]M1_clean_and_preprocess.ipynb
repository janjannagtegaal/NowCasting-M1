{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "# Nowcasting Consumer Expenditure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Dataset Description\n",
    "\n",
    "**Short Description:** The primary dataset is \"Table 1.1.5. Gross Domestic Product\" from the U.S. Bureau of Economic Analysis. It comprises seasonally adjusted quarterly U.S. Gross Domestic Product (GDP) rates in billions of dollars.\n",
    "\n",
    "**Relevance:** The dataset's detailed information on U.S. GDP over several years is integral to the project's goal of nowcasting consumption. The data's granularity and time-series nature will allow for comprehensive analysis and identification of trends, making it pivotal for the project's success.\n",
    "\n",
    "**Data frequency:** The data reflecting the economic output of the United States is crucial for analyzing economic trends and growth patterns. The presentation of data is done quarterly by the GDP component.\n",
    "\n",
    "**Location:** Available at [U.S. Bureau of Economic Analysis](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==). ([BEA](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==))\n",
    "\n",
    "**Format:** CSV\n",
    "\n",
    "**Access Method:** The dataset is readily available and can be easily accessed and downloaded directly from the U.S. Bureau of Economic Analysis website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondary Datasets\n",
    "\n",
    "##### Federal Reserve Economic Data (FRED)\n",
    "\n",
    "**Short Description:** This dataset is sourced from the Federal Reserve Bank of St. Louis's FRED macroeconomic database. It contains a variety of economic data points available at monthly intervals, with a particular focus on US GDP data. The data covers consumer spending indicators, a crucial component of the Gross Domestic Product (GDP).\n",
    "\n",
    "**Relevance**: Complements the primary dataset with additional economic indicators, useful for cross-referencing and correlation analysis.\n",
    "\n",
    "**Data frequency:** The monthly frequency of this dataset provides a more detailed temporal resolution than the primary dataset, which may reveal more immediate economic trends. This granularity will be useful in identifying more immediate proxies for nowcasting.\n",
    "\n",
    "**Estimated Size**: 0.6MB\n",
    "\n",
    "**Location**: https://research.stlouisfed.org/econ/mccracken/fred-databases/\n",
    "\n",
    "**Format**: CSV.\n",
    "\n",
    "**Access Method**: Direct download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Setup Environment and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the virtual environment by running in terminal: \n",
    "# python -m venv myenv\n",
    "# source myenv/bin/activate\n",
    "# ! source /myenv/bin/activate\n",
    "\n",
    "# ------- PIP INSTALLS -------\n",
    "# ! pip install --upgrade pip\n",
    "# ! pip install -r requirements.txt\n",
    "\n",
    "# Run the imports file\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Standard Library Imports -------\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "# from typing import List\n",
    "\n",
    "# ------- Third-Party Library Imports -------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the display options\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Load and preprocesses BEA Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loads and preprocesses** the GDP data from a CSV file. Process a DataFrame to create a structured description column.\n",
    "\n",
    "**Handling Missing Values**: Utilize median imputation for missing values, as it's less influenced by outliers and provides a more representative central tendency.\n",
    "\n",
    "**Outliers and Anomalies**: Apply Interquartile Range (IQR) or Z-score analysis to identify and address outliers. This step ensures the integrity of data by minimizing the impact of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bea_load_data import load_and_preprocess_gdp_data,create_structured_description, create_short_description, transform_date_formats\n",
    "\n",
    "file_path = './data/bea/bea_usgdp.csv'\n",
    "\n",
    "# Load and preprocess the data\n",
    "pce_df = load_and_preprocess_gdp_data(file_path)\n",
    "\n",
    "# create hierarchy for GDP data\n",
    "pce_df = create_structured_description(pce_df)\n",
    "\n",
    "# create short description for GDP data\n",
    "pce_df = create_short_description(pce_df)\n",
    "\n",
    "#extract only PCE data\n",
    "pce_df = pce_df[pce_df['short_description'].str.contains('PCE')]\n",
    "\n",
    "# Transform the date formats\n",
    "pce_df = transform_date_formats(pce_df)\n",
    "\n",
    "#save the data\n",
    "pce_df.to_csv('./results/bea/bea_pce_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pce_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Visualy inspect the BEA data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_time_series_with_iqr_and_extended_range_subplot(df, ax, column):\n",
    "    # Use the index as it is already in datetime format\n",
    "    datetime_index = df.index\n",
    "\n",
    "    # Calculate statistics\n",
    "    median = df[column].median()\n",
    "    std = df[column].std()\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = median - 2.698 * std\n",
    "    upper_whisker = median + 2.698 * std\n",
    "    \n",
    "    # Plot the time series line graph\n",
    "    ax.plot(datetime_index, df[column], marker='o', markersize=3, color='black', linewidth=1, label='PCE')\n",
    "\n",
    "    # Shade the IQR\n",
    "    ax.fill_between(datetime_index, Q1, Q3, color='grey', alpha=0.4, label='IQR')\n",
    "    \n",
    "    # Shade the extended range\n",
    "    ax.fill_between(datetime_index, lower_whisker, upper_whisker, color='lightgrey', alpha=0.3, label='Extended Range')\n",
    "    \n",
    "    # Mark potential outliers\n",
    "    outliers = df[column][(df[column] < lower_whisker) | (df[column] > upper_whisker)]\n",
    "    ax.scatter(outliers.index, outliers, color='red', zorder=5, label='Outliers')\n",
    "\n",
    "    # Add median line\n",
    "    ax.axhline(median, color='darkgreen', linestyle='--', linewidth=1.0, label='Median')\n",
    "    \n",
    "    # Add upper and lower whiskers lines\n",
    "    ax.axhline(upper_whisker, color='grey', linestyle='--', linewidth=1, label='Upper Whisker')\n",
    "    ax.axhline(lower_whisker, color='grey', linestyle='--', linewidth=1, label='Lower Whisker')\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column)\n",
    "    ax.set_title(f'{column}')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(False)\n",
    "    \n",
    "def plot_stacked_area_chart(data, ax):\n",
    "    # Plotting the stacked area chart for specified columns\n",
    "    ax.stackplot(data.index, data['PCE_Goods_Durable_goods'], data['PCE_Goods_Nondurable_goods'], data['PCE_Services'],\n",
    "                 labels=['PCE_Goods_Durable_goods','PCE_Goods_Nondurable_goods', 'PCE_Services'], \n",
    "                 alpha=0.5)\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Expenditure')\n",
    "    ax.set_title('Stacked Area Chart of PCE Components')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate of change is typically calculated as \n",
    "(\n",
    "Current Value\n",
    "−\n",
    "Previous Value\n",
    "Previous Value\n",
    ")\n",
    "×\n",
    "100\n",
    "%\n",
    "( \n",
    "Previous Value\n",
    "Current Value−Previous Value\n",
    "​\t\n",
    " )×100%, which can be easily computed using the pct_change() function in pandas, and then multiplying by 100 to convert it to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_plot(pce_df, columns_for_correlation):\n",
    "    \"\"\"\n",
    "    Function to analyze and plot data from the given DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the rate of change for each column\n",
    "    pce_real_growth = pce_df.pct_change().dropna() * 100\n",
    "\n",
    "    # Selecting the relevant columns for correlation\n",
    "    correlation_data = pce_df[columns_for_correlation]\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    plot_stacked_area_chart(pce_df, axs[0])\n",
    "    plot_time_series_with_iqr_and_extended_range_subplot(pce_real_growth, axs[1], 'PCE')\n",
    "\n",
    "    axs[1].set_title('Rate of Change for PCE')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the correlation matrix\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    return correlation_matrix\n",
    "\n",
    "# Example usage\n",
    "columns_for_correlation = ['PCE', 'PCE_Goods', 'PCE_Goods_Durable_goods','PCE_Goods_Nondurable_goods', 'PCE_Services']\n",
    "result = analyze_and_plot(pce_df, columns_for_correlation)\n",
    "\n",
    "print(f'Correlation matrix for PCE and its components:')\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Scatter Plots with Regression Lines:**\n",
    "\n",
    "Scatter plots with regression lines for each component ('PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services') against 'PCE' illustrate the linear relationships between these components and the total PCE. The regression lines provide a visual indicator of the direction, strength, and linearity of these relationships.\n",
    "\n",
    "**Interpretation of the Correlation Matrix:**\n",
    "\n",
    "The correlation matrix, particularly its visualization through the heatmap, shows the Pearson correlation coefficients between 'PCE' and its components. The coefficients near 1 indicate a very strong positive linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Loading the FRED data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_fredmd_data` function, below, performs the following actions, once for the FRED-MD dataset and once for the FRED-QD dataset:\n",
    "\n",
    "1. Based on the `vintage` argument, it downloads a particular vintage of these datasets from the base URL https://files.stlouisfed.org/files/htdocs/fred-md into the `orig_m` variable.\n",
    "2. Extracts the column describing which transformation to apply, shortname and description mappings\n",
    "3. Extracts the observation date (from the \"sasdate\" column) and uses it as the index of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fredmd_data(vintage):\n",
    "    \"\"\"\n",
    "    Loads and processes the FRED-MD data.\n",
    "    \"\"\"\n",
    "    # Define the base URL for the FRED-MD dataset\n",
    "    base_url = 'https://files.stlouisfed.org/files/htdocs/fred-md'\n",
    "\n",
    "    # Load the dataset for the specified 'vintage', dropping rows that are entirely NA\n",
    "    fred_orig = pd.read_csv(f'{base_url}/monthly/{vintage}.csv').dropna(how='all')\n",
    "\n",
    "    # Extract transformation codes (second column onwards) from the first row\n",
    "    transform_info = fred_orig.iloc[0, 1:]\n",
    "\n",
    "    # Drop the first row (containing transformation info) from the dataset\n",
    "    fred_orig = fred_orig.iloc[1:]\n",
    "\n",
    "    # Convert 'sasdate' column to a PeriodIndex with monthly frequency for time-series analysis\n",
    "    fred_orig.index = pd.PeriodIndex(fred_orig.sasdate.tolist(), freq='M')\n",
    "\n",
    "    # Remove the 'sasdate' column as it's now set as the index\n",
    "    fred_orig.drop('sasdate', axis=1, inplace=True)\n",
    "\n",
    "    # Return the processed data and the transformation information\n",
    "    return fred_orig, transform_info\n",
    "\n",
    "\n",
    "# Load data for the current vintage and unpack into original data and transformation info\n",
    "fred_orig, transform_info = load_fredmd_data(\"current\")\n",
    "\n",
    "# Save the original data\n",
    "fred_orig.to_csv('./results/fred/fred_monthly_orig.csv', index=False)\n",
    "\n",
    "fred_orig.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Mapping FRED indices to Economic Data groups for analysis per economic group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import and organize the definitions of economic variables. \n",
    "These definitions are loaded from CSV files corresponding to the FRED-MD and FRED-QD databases. \n",
    "This process ensures that we have a clear and concise understanding of each economic variable in our dataset, which is essential for accurate analysis and interpretation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Column Name Mapping\n",
    "def map_column_names(data, defn_file):\n",
    "    \"\"\"\n",
    "    Maps FRED-MD column names to their descriptions.\n",
    "    \"\"\"\n",
    "    # Load the definitions file, ignoring encoding errors\n",
    "    defn = pd.read_csv(defn_file, encoding_errors='ignore')\n",
    "\n",
    "    # Set the 'fred' column as the index of the definitions DataFrame\n",
    "    defn.index = defn.fred\n",
    "\n",
    "    # Filter the definitions to include only those variables present in the data columns\n",
    "    defn = defn.loc[data.columns.intersection(defn.fred), :]\n",
    "\n",
    "    # Create a dictionary mapping FRED-MD variable names to their descriptions\n",
    "    map_dict = defn['description'].to_dict()\n",
    "\n",
    "    # Replace the names of columns in the dataset with the descriptions from the map\n",
    "    return data[map_dict.keys()].rename(columns=map_dict),defn\n",
    "\n",
    "# Map column names for fred_original \n",
    "column_defn_file = './data/FRED/FRED_Definitions_Mapping/fredmd_definitions.csv'\n",
    "fred_orig,defn = map_column_names(fred_orig, column_defn_file)\n",
    "\n",
    "fred_orig.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we get the groups for each series from the definition files above, and then show how many of the series that we'll be using fall into each of the groups.\n",
    "\n",
    "We'll also re-order the series by group, to make it easier to interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping of variable id to group name, for monthly variables\n",
    "groups = defn[['description', 'group']].copy()\n",
    "\n",
    "# save the groups\n",
    "defn.to_csv('./results/fred/fred_indicator_mappings.csv', index=False)\n",
    "\n",
    "# Display the number of variables in each group\n",
    "(groups.groupby('group', sort=False)\n",
    "       .count()\n",
    "       .rename({'description': '# series in group'}, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Filtering the data on date range and indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only observations from the year 1980 onwards\n",
    "fred_orig = fred_orig[fred_orig.index.year >= 1960]\n",
    "\n",
    "# Filter the data to include only observations from the year 1980 onwards\n",
    "pce_df = pce_df[pce_df.index.map(lambda x: int(x[:4]) >= 1960)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Housing\tHousing Starts, Midwest\tNatural log: ln(x)\tThousands of Units\n",
    "Housing\tHousing Starts, Northeast\tNatural log: ln(x)\tThousands of Units\n",
    "Housing\tHousing Starts, South\tNatural log: ln(x)\tThousands of Units\n",
    "Housing\tHousing Starts, West\tNatural log: ln(x)\tThousands of Units\n",
    "Housing\tHousing Starts: Total New Privately Owned\tNatural log: ln(x)\tThousands of Units\n",
    "Housing\tNew Private Housing Permits (SAAR)\tNatural log: ln(x)\tThousands, Seasonally Adjusted Annual Rate\n",
    "Housing\tNew Private Housing Permits, Midwest (SAAR)\tNatural log: ln(x)\tThousands, Seasonally Adjusted Annual Rate\n",
    "Housing\tNew Private Housing Permits, Northeast (SAAR)\tNatural log: ln(x)\tThousands, Seasonally Adjusted Annual Rate\n",
    "Housing\tNew Private Housing Permits, South (SAAR)\tNatural log: ln(x)\tThousands, Seasonally Adjusted Annual Rate\n",
    "Housing\tNew Private Housing Permits, West (SAAR)\tNatural log: ln(x)\tThousands, Seasonally Adjusted Annual Rate\n",
    "\n",
    "Seasonal Adjustment: For non-seasonally adjusted data, you should consider applying seasonal adjustment first to remove seasonal effects and better isolate the underlying trends. However, as your data appears to be seasonally adjusted (as indicated by SAAR - Seasonally Adjusted Annual Rate), this step may already be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# drop the individual regional columns and just keep total 'Housing Starts: Total New Privately Owned' and 'New Private Housing Permits (SAAR)' column\n",
    "columns_to_drop = [\n",
    "    'Housing Starts, Midwest',\n",
    "    'Housing Starts, Northeast',\n",
    "    'Housing Starts, South',\n",
    "    'Housing Starts, West',\n",
    "    'New Private Housing Permits, Midwest (SAAR)',\n",
    "    'New Private Housing Permits, Northeast (SAAR)',\n",
    "    'New Private Housing Permits, South (SAAR)',\n",
    "    'New Private Housing Permits, West (SAAR)',\n",
    "    'Ratio of Help Wanted/No. Unemployed' #drop columns described by literature as not useful\n",
    "    ]\n",
    "fred_orig.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "# New Orders for Consumer Goods\n",
    "\n",
    "# Consumer Sentiment Index only last 30 years\n",
    "# fred_orig.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Frequency Alignment and Intergration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Frequency Alignment: \n",
    "</div>\n",
    "- Transform the monthly economic indices from FRED to a quarterly format to align with the BEA’s quarterly GDP data. Calculate the sum or average (as appropriate) of monthly values within each quarter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform monthly date to quarterly date in 'YYYYQX' format\n",
    "def transform_to_quarterly(date_str):\n",
    "    year, month = date_str.split('-')\n",
    "    quarter = (int(month) - 1) // 3 + 1\n",
    "    return f\"{year}Q{quarter}\"\n",
    "\n",
    "# Convert index to string to apply string methods\n",
    "fred_orig.index = fred_orig.index.astype(str)\n",
    "\n",
    "# Selecting only the last month of each quarter from the monthly dataset\n",
    "# The last month of each quarter are March (03), June (06), September (09), December (12)\n",
    "fred_orig_filtered = fred_orig[fred_orig.index.str.endswith(('03', '06', '09', '12'))]\n",
    "\n",
    "# Transform the index to the quarterly format and create a new 'Quarter' column\n",
    "fred_orig_filtered['Quarter'] = fred_orig_filtered.index.map(transform_to_quarterly)\n",
    "\n",
    "# Set the new 'Quarter' column as the index\n",
    "fred_orig_filtered.set_index('Quarter', inplace=True)\n",
    "\n",
    "# Checking the date range of the monthly dataset\n",
    "monthly_date_range = fred_orig_filtered.index.min(), pce_df.index.max()\n",
    "\n",
    "# Checking the date range of the PCE dataset\n",
    "pce_date_range = pce_df.index.min(), pce_df.index.max()\n",
    "\n",
    "monthly_date_range, pce_date_range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Quarterly Data Integration\n",
    "\n",
    "</div>\n",
    "- We will merge quarterly BEA PCE rate of change data framework with the FRED transformed quarter-over-quarter rate of change into a unified framework using pandas, ensuring seamless integration and compatibility. \n",
    "- This step is vital for consolidating different economic indicators into a single, comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns PCE_Goods\tPCE_Goods_Durable_goods\tPCE_Goods_Nondurable_goods\tPCE_Services\n",
    "pce_df.drop(['PCE_Goods', 'PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services'], axis=1, inplace=True)\n",
    "\n",
    "# Merging the datasets on the 'Quarter' column\n",
    "joined_dataset = pd.merge(pce_df, fred_orig_filtered, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Display the first few rows of the merged dataset\n",
    "joined_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dataset.to_csv(\"./results/merged_data/joined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Inspect and Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Outliers are defined as observations that deviate significantly from the series mean, specifically those that are more than 10 times the interquartile range (IQR) away from the mean.\n",
    "- To carry out this outlier removal, we've created a function named `remove_outliers`. This function identifies extreme values and replaces them with NaN (missing values). It also keeps track of the year in which each extreme value was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect meausure types for each indicator to understand how to treat outliers\n",
    "\n",
    "#map columns to measure type and remove outlier according to measure type\n",
    "measuremnet_info = pd.read_csv('./data/fredmd_information.csv')\n",
    "\n",
    "# Create a dictionary mapping FRED-MD variable names to their descriptions\n",
    "measure_type_dict = measuremnet_info.set_index('description')['measure'].to_dict()\n",
    "\n",
    "# Add \"PCE\": 'billions of dollars' to the dictionary\n",
    "measure_type_dict[\"PCE\"] = 'billions of dollars'\n",
    "\n",
    "#display the measure types in a dataframe\n",
    "pd.DataFrame.from_dict(measure_type_dict, orient='index', columns=['measure']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handles outliers in a DataFrame column based on the measure type and reports the removed outliers.\n",
    "\n",
    "Args:\n",
    "df (DataFrame): The DataFrame containing the data.\n",
    "column (str): The name of the column to process.\n",
    "measure (str): The type of measure for the column.\n",
    "\n",
    "Returns:\n",
    "DataFrame, DataFrame: The DataFrame with outliers handled, and DataFrame of removed outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, column, measure):\n",
    "    \n",
    "    outlier_values = pd.DataFrame()\n",
    "\n",
    "    if measure in ['avg dollars per hour', 'billions of 1982-84 dollars)', 'billions of dollars', \n",
    "                   'millions of dollars', 'exchange rate', 'index = 100', 'billions of chained 2012 dollars',\n",
    "                   'billions of 2012 dollars)', 'millions of 2012 dollars, deflated by core pce', \n",
    "                   'billions of dollars, adjusted for inflation and excluding government transfer payments.']:\n",
    "        # Z-score method for dollar values and indexes\n",
    "        z = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
    "        threshold = 3  # Typically, a threshold of 3 is used\n",
    "        mask = z > threshold\n",
    "        outlier_values = df.loc[mask, column].reset_index()\n",
    "        df.loc[mask, column] = np.nan\n",
    "\n",
    "    elif measure in ['avg hours', 'avg no of weeks', 'avg dollars per hour', 'thousands of units']:\n",
    "        # IQR method for averages and units\n",
    "        Q1 = df[column].quantile(0.20)\n",
    "        Q3 = df[column].quantile(0.80)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        mask = ~df[column].between(lower_bound, upper_bound)\n",
    "        outlier_values = df.loc[mask, column].reset_index()\n",
    "        df.loc[mask, column] = np.nan\n",
    "\n",
    "    elif measure in ['percent', 'ratio' ]: #'thousands of persons'\n",
    "        # Percentile-based method for percentages and ratios\n",
    "        lower_bound = df[column].quantile(0.03)\n",
    "        upper_bound = df[column].quantile(0.97)\n",
    "        mask = ~df[column].between(lower_bound, upper_bound)\n",
    "        outlier_values = df.loc[mask, column].reset_index()\n",
    "        df.loc[mask, column] = np.nan\n",
    "\n",
    "    return df, outlier_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a dictionary to store outlier data for each column\n",
    "all_outliers = {}\n",
    "columns_with_outliers = {}\n",
    "\n",
    "# Loop through each column in fred_orig and apply the outlier handling function\n",
    "for column in fred_orig.columns:\n",
    "    # Get the measure type for the column\n",
    "    measure_type = measure_type_dict.get(column)\n",
    "    if measure_type:\n",
    "        # Apply the outlier handling function\n",
    "        fred_orig, outliers = handle_outliers(fred_orig, column, measure_type)\n",
    "        \n",
    "        # Store the outliers if any\n",
    "        if not outliers.empty:\n",
    "            all_outliers[column] = outliers\n",
    "            # columns_with_outliers.append(column)\n",
    "            #add to dictionary columns_with_outliers\n",
    "            columns_with_outliers[column] = outliers.count()[1]\n",
    "\n",
    "# convert dictionary to dataframe and sort by number of outliers\n",
    "columns_with_outliers = pd.DataFrame.from_dict(columns_with_outliers, orient='index', columns=['outliers']).sort_values(by='outliers', ascending=False)\n",
    "columns_with_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Log Transformation on joined dataset for comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale:** \n",
    "\n",
    "Logarithmic transformation is used to stabilize the variance in data that exhibits exponential growth or large fluctuations. This is especially crucial for datasets like FRED's, where certain indicators can show significant variability over time. Given the information from the FRED database and their suggested transformation types, it seems reasonable to align with their expertise and apply these transformations to your dataset. This approach will save time and ensure that the data is treated consistently with established economic analysis practices.\n",
    "\n",
    "**Transformation Types (as per FRED):**\n",
    "\n",
    "1. **No Transformation (1)**: The data is used as is, without any modification.\n",
    "   \n",
    "2. **First Difference (∆x_t) (2)**: The change from one period to the next, useful for highlighting trends.\n",
    "   \n",
    "3. **Second Difference (∆^2x_t) (3)**: The change in the first difference, often used to capture acceleration or deceleration in a series.\n",
    "   \n",
    "4. **Natural Log (log(x_t)) (4)**: Useful for stabilizing variance and making exponential growth trends linear.\n",
    "   \n",
    "5. **First Difference of Log (∆ log(x_t)) (5)**: Commonly used to convert data into a stationary series, representing percentage change.\n",
    "   \n",
    "6. **Second Difference of Log (∆^2 log(x_t)) (6)**: The change in the first difference of the log, similar to the second difference but for logged data.\n",
    "   \n",
    "7. **Percentage Change from Prior Period (∆(x_t/x_t_−_1 − 1.0)) (7)**: This calculates the percentage change from the previous period, emphasizing relative changes.\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "- **Apply Transformations:** Apply FRED Transformations and use the transformation codes provided in your `fred_indicator_mappings` dataset to transform the corresponding series in your `pce_joined_dataset`.\n",
    "- This approach should streamline our analysis process and align with the methodology with FRED's established practices. \n",
    "- Additionally, it ensures that the data is treated in a manner that is suitable for economic analysis.\n",
    "-  **FRED Logarithmic Key Mapping:** We will map the transformation codes in the FREDmd_defn dataset to our dataset's indicators and then perform the necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  transformation function to handle the time column and a special case for PCE\n",
    "def modified_log_transform(column, time_column, transformation_code, column_name):\n",
    "    \"\"\"\n",
    "    Applies the specified transformation to a Pandas Series, considering the time column and special cases.\n",
    "    \"\"\"\n",
    "    # Special instruction for the PCE column\n",
    "    if column_name == \"PCE\":\n",
    "        transformation_code = 5  #6 # according to FREDs guidelines\n",
    "    # elif column_name == \"IP: Nondurable Consumer Goods\":\n",
    "    #     transformation_code = 3 #previous mapping to was still non-stationary\n",
    "    # elif (column_name == \"Housing Starts: Total New Privately Owned\") or \"New Private Housing Permits (SAAR)\":\n",
    "    #     transformation_code = 2 #previous mapping to 4 was still non-stationary\n",
    "\n",
    "    # Check if the data is quarterly based on the time column\n",
    "    mult = 4 if any(time_column.str.endswith(('Q1', 'Q2', 'Q3', 'Q4'))) else 1\n",
    "\n",
    "    if transformation_code == 1:\n",
    "        # No transformation -> Mathematical Equation: x(t)\n",
    "        # It leaves the data in its original form, without any alteration.\n",
    "        return column\n",
    "    \n",
    "    elif transformation_code == 2:\n",
    "        # First Difference -> Mathematical Equation: x(t) - x(t-1)\n",
    "        # It measures the absolute change from one period to the next, helping to detrend the data.\n",
    "        return column.diff()\n",
    "    \n",
    "    elif transformation_code == 3:\n",
    "        # Second Difference -> Mathematical Equation: (x(t) - x(t-1)) - (x(t-1) - x(t-2))\n",
    "        # It measures the change in the first difference, capturing the acceleration or deceleration in the data's movement.\n",
    "        return column.diff().diff()\n",
    "    \n",
    "    elif transformation_code == 4:\n",
    "        # Log Transformation -> Mathematical Equation: ln(x(t))\n",
    "        # It stabilizes the variance across the data series and can help make a skewed distribution more normal.\n",
    "        return np.log(column)\n",
    "    \n",
    "    elif transformation_code == 5:\n",
    "        # Log First Difference -> Mathematical Equation: 100 * (ln(x(t)) - ln(x(t-1)))\n",
    "        # It measures the growth rate from one period to the next and multiplies by 100 for percentage change.\n",
    "        # The 'mult' variable allows for scaling the growth rate if necessary.\n",
    "        return np.log(column).diff() * 100 * mult\n",
    "    \n",
    "    elif transformation_code == 6:\n",
    "        # Log Second Difference -> Mathematical Equation: 100 * ((ln(x(t)) - ln(x(t-1))) - (ln(x(t-1)) - ln(x(t-2))))\n",
    "        # It measures the change in the growth rate (change in log first difference), capturing the momentum of change.\n",
    "        # The 'mult' variable allows for scaling the change in growth rate if necessary.\n",
    "        return np.log(column).diff().diff() * 100 * mult\n",
    "    \n",
    "    elif transformation_code == 7:\n",
    "        # Exact Percent Change -> Mathematical Equation: 100 * ((x(t)/x(t-1))^mult - 1)\n",
    "        # It measures the percentage change from one period to the next, with an option to compound the change using 'mult'.\n",
    "        return ((column / column.shift(1))**mult - 1.0) * 100\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid transformation code\")\n",
    "\n",
    "\n",
    "# Create a mapping of columns to transformation codes\n",
    "transformation_mapping = defn.set_index('description')['tcode'].to_dict()\n",
    "\n",
    "# Extracting the time column\n",
    "time_column = joined_dataset.index\n",
    "\n",
    "# Applying the transformations to the dataframe\n",
    "transformed_dataset = joined_dataset.copy()\n",
    "\n",
    "for column in transformed_dataset.columns:\n",
    "    # Check if the column is in the mapping, else apply special instruction for PCE\n",
    "    tcode = transformation_mapping.get(column, None)\n",
    "    transformed_dataset[column] = modified_log_transform(transformed_dataset[column], time_column, tcode, column)\n",
    "\n",
    "# Drop the first 5 rows containing NaN values resulting from the transformation\n",
    "transformed_dataset = transformed_dataset.iloc[5:]\n",
    "\n",
    "# Displaying the first few rows of the transformed dataset\n",
    "joined_dataset = transformed_dataset\n",
    "joined_dataset.head(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Inspecting and Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check for NaN values in the dataset\n",
    "nan_summary = joined_dataset.isna().sum()\n",
    "\n",
    "# Sort the NaN summary to identify columns with the most missing values\n",
    "nan_summary_sorted = nan_summary.sort_values(ascending=False)\n",
    "\n",
    "# Display columns with NaNs and their count\n",
    "columns_with_nans = nan_summary_sorted[nan_summary_sorted > 0]\n",
    "columns_with_nans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save New Orders for Consumer Goods and Consumer Sentiment Index in seperate dataframe and drop from joined_dataset for later seperate correlation testing\n",
    "new_orders_for_consumer_goods = joined_dataset['New Orders for Consumer Goods']\n",
    "consumer_sentiment_index = joined_dataset['Consumer Sentiment Index']\n",
    "\n",
    "#drop columms from joined_dataset\n",
    "joined_dataset.drop(['New Orders for Consumer Goods', 'Consumer Sentiment Index'], axis=1, inplace=True)\n",
    "\n",
    "#save the two dataframes\n",
    "new_orders_for_consumer_goods.to_csv('./results/csi_and_new_orders/new_orders_for_consumer_goods.csv')\n",
    "consumer_sentiment_index.to_csv('./results/csi_and_new_orders/consumer_sentiment_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# def handle_and_display_missing_values(df):\n",
    "\n",
    "#     # Forward fill, then backward fill\n",
    "#     df.bfill(inplace=True)\n",
    "#     df.ffill(inplace=True)\n",
    "    \n",
    "#     # Linear interpolation\n",
    "#     #df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "#     # Recheck for NaN values after interpolation\n",
    "#     nan_summary_after_interpolation = df.isna().sum()\n",
    "#     print(\"\\nNaN values after interpolation and forward/backward fill:\")\n",
    "#     print(nan_summary_after_interpolation[nan_summary_after_interpolation > 0])\n",
    "\n",
    "#     # Median imputation for any remaining missing values\n",
    "#     if nan_summary_after_interpolation.any():\n",
    "#         imputer = SimpleImputer(strategy='median')\n",
    "#         df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "#     # Final check for NaN values\n",
    "#     nan_summary_after_all_imputations = df.isna().sum()\n",
    "#     print(\"\\nFinal NaN values summary:\")\n",
    "#     print(nan_summary_after_all_imputations[nan_summary_after_all_imputations > 0])\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # Assuming 'joined_dataset' is your DataFrame\n",
    "# handled_dataset = handle_and_display_missing_values(joined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Stationary Assesment for Joined Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Stationarity Assessment**: Using tests like the Augmented Dickey-Fuller ensures that our time series data is suitable for modelling and forecasting, as many statistical models require stationarity for valid results.\n",
    "\n",
    "**Addressing Non-Stationarity**: Techniques such as differencing or transformation will be applied to achieve stationarity, which is crucial for the accuracy and reliability of our predictive models and correlation analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Function to perform Augmented Dickey-Fuller test for stationarity\n",
    "def adf_test(series, signif=0.05):\n",
    "\n",
    "    # Replace infinities with NaNs and then fill them\n",
    "    series.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Now run the ADF test\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    p_value = result[1]\n",
    "    return p_value < signif\n",
    "\n",
    "#get join_dataset column names in a list and place in key_indicators_for_var to contain valid column names\n",
    "key_indicators_for_var = joined_dataset.columns.tolist()\n",
    "\n",
    "# Checking stationarity\n",
    "stationarity_results = {}\n",
    "for indicator in key_indicators_for_var:\n",
    "    if indicator in joined_dataset.columns:\n",
    "        stationarity_results[indicator] = adf_test(joined_dataset[indicator].dropna())\n",
    "    else:\n",
    "        print(f\"Indicator {indicator} not found in dataset\")\n",
    "\n",
    "# Print non-stationary indicators\n",
    "print(\"Non-stationary Indicators:\")\n",
    "for key, value in stationarity_results.items():\n",
    "    if not value:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treat non stationary indicators\n",
    "\n",
    "#to be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Create Additional dataset with Differencing (through Rate of Change Q-o-Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To create secondary dataset to compare different economic indicators on a common scale.\n",
    "\n",
    "**Method:** We normalize the rate of change of various indicators from one period to the next. This standardization facilitates more meaningful analysis across diverse data points, as it accounts for differences in magnitude and unit measurements.\n",
    "\n",
    "Using the rate of change as a standardization method for these variables can be quite effective, especially in the context of economic data and nowcasting models. This approach has several advantages:\n",
    "\n",
    "**Advantages of Using Rate of Change**\n",
    "\n",
    "1. **Comparability**: It allows for a more meaningful comparison across different indicators, which may have different scales and units.\n",
    "\n",
    "2. **Trend Analysis**: Rate of change emphasizes trends and growth rates, which are often more informative for economic analysis than absolute levels.\n",
    "\n",
    "3. **Stationarity**: Economic time series data often need to be stationary for effective modeling. Rates of change can help in achieving stationarity, a common requirement for many time series models.\n",
    "\n",
    "4. **Handling Non-Linearity**: Log transformations followed by calculating rates of change can linearize exponential growth patterns, making the data more suitable for linear modeling techniques.\n",
    "\n",
    "5. **Economic Relevance**: Rates of change are inherently more meaningful in economic analysis. For instance, policymakers and analysts are often more interested in the growth rate of GDP rather than its absolute level.\n",
    "\n",
    "**Considerations**\n",
    "\n",
    "However, there are a few considerations to keep in mind:\n",
    "\n",
    "- **Loss of Level Information**: By focusing on rates of change, you lose information about the absolute levels, which can sometimes be relevant.\n",
    "- **Volatility**: Rates of change can sometimes amplify volatility, especially in series with small absolute numbers.\n",
    "- **Interpretability**: Ensure that the transformed data remains interpretable and aligned with economic intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dataset_rate_of_change = joined_dataset.pct_change()\n",
    "\n",
    "joined_dataset_rate_of_change = joined_dataset_rate_of_change.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# joined_dataset_rate_of_change = handle_and_display_missing_values(joined_dataset_rate_of_change)\n",
    "\n",
    "joined_dataset_rate_of_change.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "##### Standardizing rate of change dataframe (Z-Score Normalization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rescaling the data to have a mean of zero and a standard deviation of one, we facilitate multivariate analyses and make our variables comparable in terms of variation from their mean growth. This is crucial for regression models and techniques like PCA where scale impacts the results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it and applying this only to the numeric columns\n",
    "scaled_data = scaler.fit_transform(joined_dataset_rate_of_change.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Create a new DataFrame with the scaled data and assumes that the index contains non-numeric columns like dates\n",
    "joined_dataset_rate_of_change = pd.DataFrame(scaled_data, index=joined_dataset_rate_of_change.index, columns=joined_dataset_rate_of_change.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "joined_dataset_rate_of_change.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Initial Correlation Analysis on the Transformed Data and Rate of Change Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why**: \n",
    "\n",
    "Given our scenario where we are analyzing a large number of indicators (123) across a lengthy period (1960 to 2023) to find those that best correlate with private consumption expenditure, and we want to `retain NaN` values due to their economic significance, `Spearman's rank correlation` with pairwise deletion seems to be the most appropriate. \n",
    "\n",
    "It respects the economic significance of NaN values while providing a robust correlation measure. However, we need to ensure to perform a sensitivity analysis to understand the impact of missing values on your results. Also, consider the potential for non-random missing data and its implications.\n",
    "\n",
    "**What**: \n",
    "`Spearman's rank correlation` is non-parametric and does not assume a linear relationship between variables, which can be more appropriate for economic data. It also handles NaN values by default in many implementations, like in Python's Pandas library, where it ignores pairs where either value is NaN. \n",
    "\n",
    "**Benefits**: \n",
    "\n",
    "Accounts for monotonic relationships and is less sensitive to outliers (which is relevant given that you have replaced some extreme values with NaN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spearman's rank correlation for log transformed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Spearman's rank correlation with the private consumption expenditure, handling NaNs with pairwise deletion.\n",
    "correlation_matrix = joined_dataset.corr(method='spearman')\n",
    "\n",
    "#target_correlations will have the Spearman's rank correlation coefficients\n",
    "target_correlations = correlation_matrix['PCE'].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations related to 'PCE'\n",
    "print(target_correlations.head(10))\n",
    "print('\\n')\n",
    "print(target_correlations.tail(10).sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spearman's rank correlation for rate of change data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Spearman's rank correlation with the private consumption expenditure, handling NaNs with pairwise deletion.\n",
    "correlation_matrix_rate_of_change = joined_dataset_rate_of_change.corr(method='spearman')\n",
    "\n",
    "#target_correlations will have the Spearman's rank correlation coefficients\n",
    "target_correlations_rate_of_change = correlation_matrix['PCE'].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations related to 'PCE'\n",
    "print(target_correlations_rate_of_change.head(10))\n",
    "print('\\n')\n",
    "print(target_correlations_rate_of_change.tail(10).sort_values(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "### Save the final cleaned and preprocessed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "joined_dataset.to_csv(\"./results/merged_data/joined_dataset_transformed.csv\",index=True)\n",
    "joined_dataset_rate_of_change.to_csv(\"./results/merged_data/joined_dataset_rate_of_change.csv\",index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
