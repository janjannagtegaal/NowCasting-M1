{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "# Nowcasting Consumer Expenditure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "## Uncovering Reliable Proxies for Consumer Spending Behaviour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Introduction: The Problem (Why)\n",
    "\n",
    "The current quarterly GDP reports lag in reflecting the dynamic changes in the economy, impacting decision-makers who rely on timely economic data. This project is devised to mitigate this issue by identifying high-frequency, readily updated data proxies that offer quicker insights into consumer expenditure patterns.\n",
    "\n",
    "#### 1.2. Project Scope and Objectives (What)\n",
    "\n",
    "The project's primary objective is to systematically identify, harmonise, and validate high-frequency data sources as proxies for real-time tracking of consumer expenditure in the United States. The goal is to refine these proxies to provide more immediate data on consumer spending habits, thus bridging the gap caused by the delayed reporting of official GDP figures.\n",
    "\n",
    "#### Key Questions:\n",
    "\n",
    "- Which high-frequency data sources can serve as accurate proxies for consumer spending?\n",
    "- How can we validate these proxies against established measures of consumer expenditure?\n",
    "- What techniques can we employ to ensure these proxies offer immediate and reliable insights into current consumer spending trends?\n",
    "- How will we address potential discrepancies between different data sources in terms of scale, units, or reporting standards?\n",
    "- Are there any unforeseen challenges in harmonizing data frequencies (monthly vs. quarterly) that could impact the accuracy of our analysis?\n",
    "- How can we ensure the economic relevance of our findings, beyond statistical correlations?\n",
    "- What contingency plans do we have for dealing with data anomalies or irregularities that might skew our analysis?\n",
    "\n",
    "#### 1.3. Methodology\n",
    "\n",
    "The methodology is designed to focus on data preparation and validation:\n",
    "\n",
    "- **Exploratory Data Analysis (EDA)**: To understand the characteristics and quality of the high-frequency monthly indicators and their initial relationships to consumer spending.\n",
    "- **Data Harmonization**: To transform and align the monthly indicators with the quarterly GDP data, using log transformations and adjustments for seasonality and rate of change.\n",
    "- **Proxy Validation**: To establish a correlation with established measures of consumer spending through statistical analysis, ensuring that the proxies are reliable and relevant.\n",
    "\n",
    "#### 1.4. Assumptions\n",
    "\n",
    "**Data Quality and Relevance:** We operate under the assumption that the high-frequency data from FRED and other sources accurately reflect current economic trends and consumer sentiments. However, there is an inherent risk of data bias or inaccuracy, which could impact the reliability of our findings.\n",
    "\n",
    "**Predictive Power and Relevance:** While we aim to identify effective proxies for consumer expenditure, there's a risk that these proxies may not fully capture the complexities of consumer behaviour or may not adapt swiftly to sudden economic shifts.\n",
    "\n",
    "**External Factors:** The project also assumes a stable economic environment. Sudden external shocks (like global events or policy changes) could significantly affect consumer behaviour, potentially reducing the predictive accuracy of our proxies.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Primary Dataset Description\n",
    "\n",
    "**Short Description:** The primary dataset is \"Table 1.1.5. Gross Domestic Product\" from the U.S. Bureau of Economic Analysis. It comprises seasonally adjusted quarterly U.S. Gross Domestic Product (GDP) rates in billions of dollars.\n",
    "\n",
    "**Relevance:** The dataset's detailed information on U.S. GDP over several years is integral to the project's goal of nowcasting consumption. The data's granularity and time-series nature will allow for comprehensive analysis and identification of trends, making it pivotal for the project's success.\n",
    "\n",
    "**Data frequency:** The data reflecting the economic output of the United States is crucial for analyzing economic trends and growth patterns. The presentation of data is done quarterly by the GDP component.\n",
    "\n",
    "**Location:** Available at [U.S. Bureau of Economic Analysis](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==). ([BEA](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==))\n",
    "\n",
    "**Format:** CSV\n",
    "\n",
    "**Access Method:** The dataset is readily available and can be easily accessed and downloaded directly from the U.S. Bureau of Economic Analysis website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Secondary Datasets\n",
    "\n",
    "##### Federal Reserve Economic Data (FRED)\n",
    "\n",
    "**Short Description:** This dataset is sourced from the Federal Reserve Bank of St. Louis's FRED macroeconomic database. It contains a variety of economic data points available at monthly intervals, with a particular focus on US GDP data. The data covers consumer spending indicators, a crucial component of the Gross Domestic Product (GDP).\n",
    "\n",
    "**Relevance**: Complements the primary dataset with additional economic indicators, useful for cross-referencing and correlation analysis.\n",
    "\n",
    "**Data frequency:** The monthly frequency of this dataset provides a more detailed temporal resolution than the primary dataset, which may reveal more immediate economic trends. This granularity will be useful in identifying more immediate proxies for nowcasting.\n",
    "\n",
    "**Estimated Size**: 0.6MB\n",
    "\n",
    "**Location**: https://research.stlouisfed.org/econ/mccracken/fred-databases/\n",
    "\n",
    "**Format**: CSV.\n",
    "\n",
    "**Access Method**: Direct download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "## Setup Environment and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the virtual environment by running in terminal: \n",
    "# python -m venv myenv\n",
    "# source myenv/bin/activate\n",
    "# ! source /myenv/bin/activate\n",
    "\n",
    "# # ------- PIP INSTALLS -------\n",
    "# ! pip install --upgrade pip\n",
    "# ! python3.10 -m pip install --upgrade pip\n",
    "# ! pip install -r requirements.txt\n",
    "# ! pip install pandas\n",
    "# ! pip install matplotlib\n",
    "# ! pip install seaborn\n",
    "# ! pip install numpy\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install scipy\n",
    "# ! pip install statsmodels\n",
    "\n",
    "# Run the imports file\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "# import os\n",
    "\n",
    "# def extract_imports_from_notebook(notebook_path):\n",
    "#     with open(notebook_path, 'r', encoding='utf-8') as file:\n",
    "#         nb = nbformat.read(file, as_version=4)\n",
    "\n",
    "#     imports = set()\n",
    "#     for cell in nb['cells']:\n",
    "#         if cell['cell_type'] == 'code':\n",
    "#             lines = cell['source'].split('\\n')\n",
    "#             for line in lines:\n",
    "#                 if line.startswith('import ') or line.startswith('from '):\n",
    "#                     imports.add(line.split()[1].split('.')[0])\n",
    "\n",
    "#     return imports\n",
    "\n",
    "# # Path to Jupyter notebook\n",
    "# notebook_path = './M1_NowCasting_Consumer_Exp.ipynb'\n",
    "# imports = extract_imports_from_notebook(notebook_path)\n",
    "\n",
    "# # Print out the unique imports\n",
    "# print(\"Libraries to install:\")\n",
    "# for lib in imports:\n",
    "#     print(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Standard Library Imports -------\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "# ------- Third-Party Library Imports -------\n",
    "# Data handling and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utility and display modules\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the display options\n",
    "# pd.set_option('display.max_rows', None)  \n",
    "# pd.set_option('display.max_columns', None)  \n",
    "# pd.set_option('display.width', None)  \n",
    "# pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;\">\n",
    "\n",
    "## Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;\">\n",
    "\n",
    "### Load and Pre Process BEAU Quarterly GDP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loads and preprocesses** the GDP data from a CSV file. Process a DataFrame to create a structured description column.\n",
    "\n",
    "**Handling Missing Values**: Utilize median imputation for missing values, as it's less influenced by outliers and provides a more representative central tendency.\n",
    "\n",
    "**Outliers and Anomalies**: Apply Interquartile Range (IQR) or Z-score analysis to identify and address outliers. This step ensures the integrity of data by minimizing the impact of extreme values.\n",
    "\n",
    "**Data Type Standardization**: Use Python's Pandas library to standardise data formats and types across datasets. This step is crucial to ensure consistency, particularly when dealing with various formats like percentages, counts, and currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_gdp_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the GDP data from a CSV file.\n",
    "    Args:file_path (str): The path to the CSV file containing GDP data.\n",
    "    Returns:pandas.DataFrame: Preprocessed GDP data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data with specified rows to skip and number of rows to read\n",
    "    df = pd.read_csv(file_path, skiprows=3, nrows=28)\n",
    "\n",
    "    # Drop the first column (unnecessary or identifier column)\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Rename the first column as 'description'\n",
    "    df.rename(columns={df.columns[0]: 'description'}, inplace=True)\n",
    "\n",
    "    # Remove any characters after (and including) the \".\" in column names\n",
    "    df.columns = df.columns.str.replace(r'\\..*', '', regex=True)\n",
    "\n",
    "    # Concatenate the column names with the first row values, handling NaNs\n",
    "    df.columns = df.columns + \" \" + df.iloc[0].fillna('')\n",
    "\n",
    "    # Drop the first row as it's now part of the column names\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Correct any trailing space issues in the 'description' column name\n",
    "    df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_description(df):\n",
    "    \"\"\"\n",
    "    Process a DataFrame to create a structured description column.\n",
    "    \n",
    "    This function takes a DataFrame with a 'description' column and adds structure to it\n",
    "    based on indentation levels, indicating hierarchical relationships.\n",
    "\n",
    "    Parameters:gdp_df (DataFrame): A pandas DataFrame with a column named 'description'.\n",
    "    Returns:DataFrame: The modified DataFrame with a structured 'description' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to determine the indentation level (number of leading spaces)\n",
    "    def indentation_level(s):\n",
    "        \"\"\"Return the number of leading spaces in a string, indicating the indentation level.\"\"\"\n",
    "        return len(s) - len(s.lstrip())\n",
    "\n",
    "    # Apply the function to find indentation levels\n",
    "    df['indentation'] = df['description'].apply(indentation_level)\n",
    "\n",
    "    # Initialize an empty list to store the new structured names\n",
    "    structured_names = []\n",
    "    current_parent = \"\"\n",
    "    current_subparent = \"\"\n",
    "\n",
    "    # Iterate through the DataFrame to construct the hierarchical names\n",
    "    for index, row in df.iterrows():\n",
    "        if row['indentation'] == 0:\n",
    "            name = row['description'].strip()\n",
    "            current_parent = name\n",
    "        elif row['indentation'] == 4:\n",
    "            name = f\"{current_parent} : {row['description'].strip()}\"\n",
    "            current_subparent = row['description'].strip()\n",
    "        elif row['indentation'] == 8:\n",
    "            name = f\"{current_parent} : {current_subparent} : {row['description'].strip()}\"\n",
    "        else:\n",
    "            name = row['description'].strip()\n",
    "\n",
    "        structured_names.append(name)\n",
    "\n",
    "    # Assigning the structured names to the 'description' column\n",
    "    df['description'] = structured_names\n",
    "\n",
    "    # Dropping the 'indentation' column as it's no longer needed\n",
    "    df.drop('indentation', axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_short_description(df):\n",
    "    \"\"\"\n",
    "    Create a column 'short_description' in the gdp_df DataFrame with abbreviated descriptions.\n",
    "    Parameters:gdp_df (DataFrame): A DataFrame containing GDP data with a column 'description'.\n",
    "    Returns:DataFrame: The modified DataFrame including a new 'short_description' column.\n",
    "    \"\"\"\n",
    "    def abbreviate_description(desc):\n",
    "\n",
    "        # Define a mapping from full descriptions to their abbreviations\n",
    "        abbreviations = {\n",
    "            \"Gross domestic product\": \"GDP\",\n",
    "            \"Personal consumption expenditures\": \"PCE\",\n",
    "            \"Gross private domestic investment\": \"GPDI\",\n",
    "            \"Net exports of goods and services\": \"NXGS\",\n",
    "            \"Government consumption expenditures and gross investment\": \"GCEGI\",\n",
    "        }\n",
    "\n",
    "        # Split the description into parts and abbreviate each part\n",
    "        parts = desc.split(\" : \")\n",
    "        abbreviated_parts = [abbreviations.get(part, part) for part in parts]\n",
    "\n",
    "        # Join the abbreviated parts and replace spaces with underscores\n",
    "        abrev_descr = \"_\".join(abbreviated_parts).replace(' ', '_')\n",
    "        \n",
    "        # remove all leading \"_\" characters\n",
    "        abrev_descr = abrev_descr.lstrip('_')\n",
    "        \n",
    "        #remove leading and trailing spaces from the description column\n",
    "        abrev_descr = abrev_descr.strip()\n",
    "        \n",
    "        return abrev_descr\n",
    "\n",
    "    # Apply the abbreviation function to each description\n",
    "    df['short_description'] = df['description'].apply(abbreviate_description)\n",
    "    df['description'] = df['description'].str.lstrip(\" :\").str.strip()\n",
    "\n",
    "    # Insert the new column 'short_description' right after the 'description' column\n",
    "    description_index = df.columns.get_loc(\"description\")\n",
    "    df.insert(description_index + 1, 'short_description', df.pop('short_description'))\n",
    "    \n",
    "    #drop the 'description' column\n",
    "    df.drop('description', axis=1, inplace=True)\n",
    "    \n",
    "    #move last row to after 1st row fo readability\n",
    "    last_row = df.iloc[-1].copy()\n",
    "    df = df.iloc[:-1]\n",
    "    df = pd.concat([df.iloc[:1], last_row.to_frame().T, df.iloc[1:]]).reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_formats(df):\n",
    "    # Step 1: Extract only non-date columns\n",
    "    non_date_columns = df.columns[:1]  # Assuming first column is non-date column\n",
    "\n",
    "    # Step 2: Extract and transform date columns\n",
    "    date_columns = df.columns[1:]  # Date columns start from the 2nd column\n",
    "\n",
    "    # Function to convert quarter to last date of the quarter\n",
    "    def quarter_to_date(q):\n",
    "        year, quarter = q.split(' Q')\n",
    "        year = int(year)\n",
    "        if quarter == '1':\n",
    "            return f\"{year}-03-31\"\n",
    "        elif quarter == '2':\n",
    "            return f\"{year}-06-30\"\n",
    "        elif quarter == '3':\n",
    "            return f\"{year}-09-30\"\n",
    "        elif quarter == '4':\n",
    "            return f\"{year}-12-31\"\n",
    "\n",
    "    # Apply this function to each of the date columns\n",
    "    transformed_date_columns = [quarter_to_date(col) for col in date_columns]\n",
    "\n",
    "    # Step 3: Combine the columns back together\n",
    "    df.columns = list(non_date_columns) + transformed_date_columns\n",
    "\n",
    "    # Transpose the dataset for easier manipulation (columns become rows)\n",
    "    df = df.set_index('short_description').transpose()\n",
    "\n",
    "    # Converting the index to datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    df.index.freq = 'Q'\n",
    "\n",
    "    # Convert all columns to numeric\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    \"\"\"\n",
    "    Replaces outliers in a DataFrame with NaN based on IQR.\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the mask using the typical IQR criterion\n",
    "    mask = (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))\n",
    "\n",
    "    # Replace extreme values with NaN and store the corresponding values\n",
    "    extreme_values = {}\n",
    "    for column in data.columns:\n",
    "        extreme_values[column] = data[column][mask[column]].dropna().reset_index()\n",
    "\n",
    "    data[mask] = np.nan\n",
    "    return data, extreme_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_BEA(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the GDP data from a CSV file.\n",
    "    \"\"\"\n",
    "    df = load_and_preprocess_gdp_data(file_path)\n",
    "\n",
    "    df = create_structured_description(df)\n",
    "    df = create_short_description(df)\n",
    "\n",
    "    # extract only PCE data\n",
    "    df = df[df['short_description'].str.contains('PCE')]\n",
    "\n",
    "    # Transform the date formats and remove outliers\n",
    "    df = transform_date_formats(df)\n",
    "    df, extreme_values = remove_outliers(df)\n",
    "\n",
    "    # Print the column names and values where outliers were found\n",
    "    for column, values_df in extreme_values.items():\n",
    "        if not values_df.empty:\n",
    "            print(f\"Extreme values for {column}:\")\n",
    "            print(values_df)\n",
    "        else:\n",
    "            print(f\"No extreme values for {column}\")\n",
    "            \n",
    "    return df, extreme_values\n",
    "\n",
    "file_path = './data/bea/bea_usgdp.csv'\n",
    "\n",
    "bea_pce, extreme_values = load_and_preprocess_BEA(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_pce.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;padding: 5px;\">\n",
    "\n",
    "### Load and Pre Process FRED monthly  dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;padding: 5px;\">\n",
    "\n",
    "##### Loading the FRED data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_fredmd_data` function, below, performs the following actions, once for the FRED-MD dataset and once for the FRED-QD dataset:\n",
    "\n",
    "1. Based on the `vintage` argument, it downloads a particular vintage of these datasets from the base URL https://files.stlouisfed.org/files/htdocs/fred-md into the `orig_[m|q]` variable.\n",
    "2. Extracts the column describing which transformation to apply into the `transform_[m|q]` (and, for the quarterly dataset, also extracts the column describing which factor an earlier paper assigned each variable to).\n",
    "3. Extracts the observation date (from the \"sasdate\" column) and uses it as the index of the dataset.\n",
    "4. Applies the transformations from step (2).\n",
    "5. Removes outliers for the period 1959-01 through 2019-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fredmd_data(vintage):\n",
    "    \"\"\"\n",
    "    Loads and processes the FRED-MD data.\n",
    "    \"\"\"\n",
    "    # Define the base URL for the FRED-MD dataset\n",
    "    base_url = 'https://files.stlouisfed.org/files/htdocs/fred-md'\n",
    "\n",
    "    # Load the dataset for the specified 'vintage', dropping rows that are entirely NA\n",
    "    fred_orig = pd.read_csv(f'{base_url}/monthly/{vintage}.csv').dropna(how='all')\n",
    "\n",
    "    # Extract transformation codes (second column onwards) from the first row\n",
    "    transform_info = fred_orig.iloc[0, 1:]\n",
    "\n",
    "    # Drop the first row (containing transformation info) from the dataset\n",
    "    fred_orig = fred_orig.iloc[1:]\n",
    "\n",
    "    # Convert 'sasdate' column to a PeriodIndex with monthly frequency for time-series analysis\n",
    "    fred_orig.index = pd.PeriodIndex(fred_orig.sasdate.tolist(), freq='M')\n",
    "\n",
    "    # Remove the 'sasdate' column as it's now set as the index\n",
    "    fred_orig.drop('sasdate', axis=1, inplace=True)\n",
    "\n",
    "    # Return the processed data and the transformation information\n",
    "    return fred_orig, transform_info\n",
    "\n",
    "# Load data for the current vintage and unpack into original data and transformation info\n",
    "fred_orig, transform_info = load_fredmd_data(\"current\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;padding: 5px;\">\n",
    "\n",
    "**Mapping FRED indices to Economic Data groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation*\n",
    "\n",
    "In this section, we import and organize the definitions of economic variables. \n",
    "These definitions are loaded from CSV files corresponding to the FRED-MD and FRED-QD databases. \n",
    "This process ensures that we have a clear and concise understanding of each economic variable in our dataset, which is essential for accurate analysis and interpretation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Column Name Mapping\n",
    "def map_column_names(data, Fredmd_defn):\n",
    "    \"\"\"\n",
    "    Maps FRED-MD column names to their descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the 'fred' column as the index of the definitions DataFrame\n",
    "    Fredmd_defn.index = Fredmd_defn.fred\n",
    "\n",
    "    # Filter the definitions to include only those variables present in the data columns\n",
    "    Fredmd_defn = Fredmd_defn.loc[data.columns.intersection(Fredmd_defn.fred), :]\n",
    "\n",
    "    # Create a dictionary mapping FRED-MD variable names to their descriptions\n",
    "    map_dict = Fredmd_defn['description'].to_dict()\n",
    "\n",
    "    # Replace the names of columns in the dataset with the descriptions from the map\n",
    "    return data[map_dict.keys()].rename(columns=map_dict)\n",
    "\n",
    "# Map column names for fred_original \n",
    "\n",
    "column_defn_file = './data/FRED/FRED_Definitions_Mapping/fredmd_definitions.csv'\n",
    "Fredmd_defn = pd.read_csv(column_defn_file, encoding_errors='ignore')\n",
    "\n",
    "fred_orig = map_column_names(fred_orig, Fredmd_defn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_orig.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fredmd_defn.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_codes_function(transform_info, Fredmd_defn):\n",
    "    \"\"\"\n",
    "    Transforms the provided information into a DataFrame and renames the columns.\n",
    "    Returns:DataFrame: Transformed and renamed DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert transform_info to a DataFrame and transpose it\n",
    "    transformed_df = transform_info.to_frame().T\n",
    "\n",
    "    # Map the column names using provided definitions\n",
    "    transformed_df = map_column_names(transformed_df, Fredmd_defn).T\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "# Example usage\n",
    "transformed_codes = transform_codes_function(transform_info, Fredmd_defn)\n",
    "transformed_codes = transformed_codes.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the methodology outlined in McCracken and Ng (2016), I have implemented a process to remove outliers from the dataset. \n",
    "\n",
    "- These outliers are defined as observations that deviate significantly from the series mean, specifically those that are more than 10 times the interquartile range (IQR) away from the mean.\n",
    "- However, it's worth noting that during the first half of 2020, there are numerous series containing extreme observations. These extreme values are likely to contain valuable information about the real PCE in 2020. Therefore, I have chosen to apply the outlier removal only to the period from January 1959 to December 2019.\n",
    "- To carry out this outlier removal, we've created a function named `remove_outliers`. This function identifies extreme values and replaces them with NaN (missing values). It also keeps track of the year in which each extreme value was removed.\n",
    "\n",
    "Let's proceed by implementing the `remove_outliers` function and printing out the extreme values along with their corresponding years.\n",
    "https://playfairdata.com/3-creative-ways-to-visualize-outliers-in-tableau/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "fred_orig, extreme_values = remove_outliers(fred_orig) # Remove outliers for a specific period\n",
    "\n",
    "# Print the column names and values where outliers were found\n",
    "# for column, values_df in extreme_values.items():\n",
    "#     if not values_df.empty:\n",
    "#         print(f\"Extreme values for {column}:\")\n",
    "#         print(values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we get the groups for each series from the definition files above, and then show how many of the series that we'll be using fall into each of the groups.\n",
    "\n",
    "We'll also re-order the series by group, to make it easier to interpret the results.\n",
    "\n",
    "Since we're including the quarterly real GDP variable in our analysis, we need to assign it to one of the groups in the monthly dataset. It fits best in the \"Output and income\" group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "#### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "fred_orig.to_csv(\"./results/monthly.csv\")\n",
    "bea_pce.to_csv(\"./results/pce.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "### Data Harmonization and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####     Filter the FRED and BEA PCE for a set date range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_year(fred_orig, bea_pce, year=2000):\n",
    "    \"\"\"\n",
    "    Filters the FRED and BEA PCE datasets based on the specified year.(inclusive)\n",
    "    \"\"\"\n",
    "    fred_filtered = fred_orig[fred_orig.index.year >= year]\n",
    "    bea_pce_filtered = bea_pce[bea_pce.index.year >= year]\n",
    "\n",
    "    return fred_filtered, bea_pce_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monthly Rate of Change: \n",
    "\n",
    "For indices that are better represented through changes (e.g., stock indices, employment rates), calculate the month-over-month rate of change post-log transformation. This helps to highlight immediate shifts in economic activities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rate_of_change(fred_data, bea_pce_data):\n",
    "    \"\"\"\n",
    "    Calculates the rate of change for FRED and BEA PCE datasets.\n",
    "    \"\"\"\n",
    "    # Calculate the month-over-month rate of change for the FRED dataset\n",
    "    fred_rate_of_change = fred_data.pct_change()\n",
    "\n",
    "    # Calculate the quarter-over-quarter rate of change for the PCE dataset\n",
    "    pce_rate_of_change = bea_pce_data.pct_change()\n",
    "\n",
    "    return fred_rate_of_change, pce_rate_of_change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency Alignment: \n",
    "- Transform the monthly economic indices from FRED to a quarterly format to align with the BEA’s quarterly GDP data. Calculate the sum or average (as appropriate) of monthly values within each quarter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_allignment(fred_rate_of_change, pce_rate_of_change):\n",
    "    \"\"\"\n",
    "    Transform the monthly economic indices from FRED to a quarterly format to align with the BEA’s quarterly GDP data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the monthly rate of change data to quarterly using the average as the aggregation method\n",
    "    fred_alligned = fred_rate_of_change.resample('Q').mean()\n",
    "\n",
    "    # Convert DateTimeIndex to PeriodIndex with quarterly frequency for PCE dataset\n",
    "    pce_rate_of_change.index = pce_rate_of_change.index.to_period('Q')\n",
    "    pce_alligned = pce_rate_of_change\n",
    "\n",
    "    return fred_alligned, pce_alligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, filter the data\n",
    "fred_filtered, bea_pce_filtered = filter_data_by_year(fred_orig, bea_pce, 2000)\n",
    "\n",
    "# Then, calculate the rate of change\n",
    "fred_rate_of_change, pce_rate_of_change = calculate_rate_of_change(fred_filtered, bea_pce_filtered)\n",
    "\n",
    "# Then, allign the date frequencies\n",
    "fred_alligned, pce_alligned = frequency_allignment(fred_rate_of_change, pce_rate_of_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pce_rate_of_change.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fred_rate_of_change.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### inspect PCE and FRED for Variance, Skewness, Kurtosis, distribution inspection for possible transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_with_iqr_and_extended_range(data, column):\n",
    "    # Ensure the index is in datetime format for matplotlib to plot correctly\n",
    "    datetime_index = pd.to_datetime(data.index.to_timestamp())\n",
    "\n",
    "    # Calculate statistics\n",
    "    median = data[column].median()\n",
    "    std = data[column].std()\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = median - 2.698 * std\n",
    "    upper_whisker = median + 2.698 * std\n",
    "    \n",
    "    # Plot the time series line graph\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(datetime_index, data[column], marker='o', markersize=3,color='blue', linewidth=1.5, label='PCE')\n",
    "\n",
    "    # Shade the IQR\n",
    "    plt.fill_between(datetime_index, Q1, Q3, color='grey', alpha=0.3, label='IQR')\n",
    "    \n",
    "    # Shade the extended range\n",
    "    plt.fill_between(datetime_index, lower_whisker, upper_whisker, color='lightgrey', alpha=0.2, label='Extended Range')\n",
    "    \n",
    "    # Mark potential outliers\n",
    "    outliers = data[column][(data[column] < lower_whisker) | (data[column] > upper_whisker)]\n",
    "    plt.scatter(outliers.index, outliers, color='red', zorder=5, label='Outliers')\n",
    "\n",
    "    # Add median line\n",
    "    plt.axhline(median, color='green', linestyle='--', linewidth=2, label='Median')\n",
    "    \n",
    "    # add upper and lower whiskers lines\n",
    "    plt.axhline(upper_whisker, color='grey', linestyle='--', linewidth=1, label='Upper Whisker')\n",
    "    plt.axhline(lower_whisker, color='grey', linestyle='--', linewidth=1, label='Lower Whisker')\n",
    "\n",
    "    # Annotate the median and quartiles\n",
    "    # plt.text(datetime_index[0], median, ' Median', va='center', ha='right', backgroundcolor='w')\n",
    "    # plt.text(datetime_index[0], Q1, ' Q1', va='center', ha='right', backgroundcolor='w')\n",
    "    # plt.text(datetime_index[0], Q3, ' Q3', va='center', ha='right', backgroundcolor='w')\n",
    "    # plt.text(datetime_index[0], upper_whisker, ' Upper Whisker', va='center', ha='right') #, backgroundcolor='w')\n",
    "    # plt.text(datetime_index[0], lower_whisker, ' Lower Whisker', va='center', ha='right', backgroundcolor='w')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f'Time Series with IQR and Extended Range for {column}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the chart\n",
    "for col in pce_alligned.columns:\n",
    "    plot_time_series_with_iqr_and_extended_range(pce_alligned, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution_stats(df):\n",
    "    # Calculate skewness, kurtosis, and variance for each column\n",
    "    \n",
    "    stats_df = pd.DataFrame(index=df.columns, \n",
    "                            columns=['Skewness', 'Kurtosis', 'Variance', \n",
    "                                     'Interpretation', 'Transformation', 'Visualization'])\n",
    "\n",
    "    for column in df.columns:\n",
    "        stats_df.at[column, 'Skewness'] = df[column].skew()\n",
    "        stats_df.at[column, 'Kurtosis'] = df[column].kurtosis()\n",
    "        stats_df.at[column, 'Variance'] = df[column].var()\n",
    "\n",
    "        # Interpretation of skewness and kurtosis\n",
    "        skew = stats_df.at[column, 'Skewness']\n",
    "        kurt = stats_df.at[column, 'Kurtosis']\n",
    "        transformation = \"None\"\n",
    "        log_transformation = \"None\"\n",
    "        visualization = \"Histogram or Boxplot\"\n",
    "\n",
    "        if np.abs(skew) < 0.5:\n",
    "            interpretation = 'Fairly Symmetrical'\n",
    "            if df[column].min() >= 0:\n",
    "                transformation = 'Log'\n",
    "        elif skew >= 0.5:\n",
    "            interpretation = 'Right Skewed'\n",
    "            transformation = 'Square Root or Log'\n",
    "        elif skew <= -0.5:\n",
    "            interpretation = 'Left Skewed'\n",
    "            transformation = 'Square or Cube'\n",
    "\n",
    "        if kurt > 3:\n",
    "            interpretation += \", Heavy Tails\"\n",
    "            visualization = \"Boxplot for Outliers\"\n",
    "\n",
    "        stats_df.at[column, 'Interpretation'] = interpretation\n",
    "        stats_df.at[column, 'Transformation'] = transformation\n",
    "        stats_df.at[column, 'Visualization'] = visualization\n",
    "\n",
    "    # Sort the DataFrame based on the absolute skewness\n",
    "    stats_df['Absolute Skewness'] = stats_df['Skewness'].abs()\n",
    "    sorted_stats = stats_df.sort_values(by='Absolute Skewness', ascending=False)\n",
    "\n",
    "    return sorted_stats.drop('Absolute Skewness', axis=1)\n",
    "\n",
    "# Example usage with your DataFrame\n",
    "distribution_stats = calculate_distribution_stats(fred_rate_of_change)\n",
    "distribution_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import math\n",
    "\n",
    "# # Function to plot small multiples of histograms for a selected number of columns from a DataFrame\n",
    "# def plot_distributions(df, num_columns=10):\n",
    "#     num_rows = math.ceil(len(df.columns) / num_columns)\n",
    "#     plt.figure(figsize=(20, 3 * num_rows))\n",
    "    \n",
    "#     for i, column in enumerate(df.columns):\n",
    "#         plt.subplot(num_rows, num_columns, i + 1)\n",
    "#         sns.histplot(df[column], kde=True, bins=30)\n",
    "#         plt.title(column,fontsize = 8)\n",
    "#         plt.ylabel('Frequency', fontsize=8)\n",
    "#         plt.xlabel('')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Plotting small multiples of histograms for all columns in your DataFrame\n",
    "# plot_distributions(pce_rate_of_change, num_columns=10)\n",
    "# plot_distributions(fred_quarterly_rate_of_change)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#DC143C\">\n",
    "\n",
    "##### Log Transformation for Monthly Data: IN PROGRESS\n",
    "- Implement logarithmic transformations to stabilize the variance in monthly data that exhibit exponential growth or large fluctuations. This step is particularly important for FRED data (FRED provides a logarithmic key mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(column, transformation_code):\n",
    "#     \"\"\"\n",
    "#     Applies the specified transformation to a Pandas Series.\n",
    "#     Transformation Codes from FRED suggested Description:\n",
    "#     1. No Transformation, 2. First Difference, 3. Second Difference,\n",
    "#     4. Log Transformation, 5. Log First Difference, 6. Log Second Difference,\n",
    "#     7. Exact Percent Change\n",
    "#     \"\"\"\n",
    "#     # Multiplier for quarterly data; if data is quarterly, multiply by 4, else 1\n",
    "#     mult = 4 if column.index.freqstr[0] == 'Q' else 1\n",
    "\n",
    "#     if transformation_code == 1:\n",
    "#         # No transformation, return the column as is\n",
    "#         return column\n",
    "#     if transformation_code == 2:\n",
    "#         # First Difference: Subtract each element from its predecessor\n",
    "#         # Useful for converting a series to its change values\n",
    "#         return column.diff()\n",
    "#     if transformation_code == 3:\n",
    "#         # Second Difference: Apply first difference twice\n",
    "#         # Useful when first difference is insufficient to achieve stationarity\n",
    "#         return column.diff().diff()\n",
    "#     if transformation_code == 4:\n",
    "#         # Log Transformation: Apply natural logarithm\n",
    "#         # Useful for data with exponential growth patterns\n",
    "#         return np.log(column)\n",
    "#     if transformation_code == 5:\n",
    "#         # Log First Difference: Apply log transformation, then first difference\n",
    "#         # Multiplied by 100 for percentage change, especially useful for financial data\n",
    "#         return np.log(column).diff() * 100 * mult\n",
    "#     if transformation_code == 6:\n",
    "#         # Log Second Difference: Apply log transformation, then second difference\n",
    "#         # Similar to Code 5 but provides a more refined measure of change\n",
    "#         return np.log(column).diff().diff() * 100 * mult\n",
    "#     if transformation_code == 7:\n",
    "#         # Exact Percent Change: Calculate the percentage change from one period to the next\n",
    "#         # Useful for directly understanding growth rates\n",
    "#         return ((column / column.shift(1))**mult - 1.0) * 100\n",
    "\n",
    "# # Transformation Code 5 for 'PCE', and Code 6 for the rest\n",
    "# pce_df_transformed = pce_rate_of_change.apply(lambda col: transform(col, 5 if col.name == 'PCE' else 6))\n",
    "# # Apply log transformations using original column names according to FRED guidelines\n",
    "# # fred_log_transform = fred_quarterly_rate_of_change.apply(lambda col: transform(col, transform_info[col.name][0]))\n",
    "# pce_df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fred_rate_of_change.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty DataFrame to store the transformed data\n",
    "# fred_log_transform = pd.DataFrame(index=fred_rate_of_change.index)\n",
    "\n",
    "# # Iterate through each column in fred_quarterly_rate_of_change\n",
    "# for column_name in fred_rate_of_change.columns:\n",
    "#     # Fetch the transformation code for the current column from transform_info\n",
    "#     transformation_code = transformed_codes.at[0, column_name]\n",
    "\n",
    "#     # Apply the transformation using the transform function\n",
    "#     transformed_column = transform(fred_rate_of_change[column_name], transformation_code)\n",
    "\n",
    "#     # Store the transformed column in the new DataFrame\n",
    "#     fred_log_transform[column_name] = transformed_column\n",
    "\n",
    "# fred_log_transform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### **Seasonal Adjustments**: \n",
    "- Adjust high-frequency data for seasonality, if necessary, to isolate the core economic trends from regular seasonal patterns. This step will make the data more representative of general economic behaviours, irrespective of seasonal influences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Quarterly Integration**: \n",
    "- Integrate the monthly indices into the quarterly GDP data framework. For BEA's GDP data, represent them as absolute figures or calculate the quarter-over-quarter rate of change if it aligns better with our analysis objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Final Aggregation and Comparison**: \n",
    "- Ensure that the final format of both datasets (quarterly GDP and monthly indices) is compatible for direct comparison. This could involve representing both datasets as rates of change or absolute figures based on what is most meaningful for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4.3 Data Integration and Quality Assurance*\n",
    "\n",
    "**Data Integration**: We will merge various datasets into a unified framework using pandas, ensuring seamless integration and compatibility. This step is vital for consolidating different economic indicators into a single, comprehensive analysis.\n",
    "\n",
    "**Final anomaly Detection and Correction**: Employing statistical methods to detect and correct anomalies ensures that our analysis is based on accurate and representative data, free from distortions that could lead to erroneous conclusions.\n",
    "\n",
    "**Consistency Checks**: Conducting thorough checks for data consistency, especially when integrating diverse data sources, is essential to validate the reliability and accuracy of our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Advanced Data Handling and Analysis\n",
    "\n",
    "**Standardisation of Growth Rates**: Standardizing growth rates enables us to compare different economic indicators on a common scale, facilitating a more meaningful analysis across various data points.\n",
    "\n",
    "**Stationarity Assessment**: Using tests like the Augmented Dickey-Fuller ensures that our time series data is suitable for modelling and forecasting, as many statistical models require stationarity for valid results.\n",
    "\n",
    "**Addressing Non-Stationarity**: Techniques such as differencing or transformation will be applied to achieve stationarity, which is crucial for the accuracy and reliability of our predictive models and correlation analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;padding: 5px;\">\n",
    "\n",
    "## 5. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Performed early in the project to get an overview of the data's characteristics. This step is crucial for identifying the most relevant variables for analysis, understanding the data's basic structure, and ensuring that hypotheses are grounded in both statistical findings and economic logic.\n",
    "\n",
    "- **Technique**: Using statistical tools to summarise the data, visualising distributions with histograms, identifying correlations with scatter plots, and detecting patterns and outliers with box plots.\n",
    "- **Objective**: To gain an initial understanding of data trends, outliers, and correlations and to identify any anomalies or irregularities that may influence further analysis. Hereafter we will incorporate economic theories to hypothesise potential relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "5.3 Seasonality Adjustment Analysis\n",
    "\n",
    "Conducted post-EDA to refine the data for more accurate correlation analysis. Seasonality adjustment is essential for preventing seasonal patterns from distorting the true economic trends.\n",
    "\n",
    "- \n",
    "- **Technique**: Applying time-series decomposition methods to separate the data into trend, seasonal, and residual components and then adjusting for these seasonal effects.\n",
    "- **Objective**: To accurately capture the underlying trends in consumer spending by removing repetitive seasonal patterns, which are regular but not necessarily related to the economic indicators of interest. *While crucial, we have to ensure this doesn't lead to an overly complex focus on time-series analysis techniques unless they are directly relevant to identifying proxies.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Correlation and Proxy Validation Analysis\n",
    "\n",
    "Implemented after seasonality adjustments to ensure that the relationships being analysed and the proxies being identified are not influenced by seasonal fluctuations and confirm that identified relationships are economically plausible as well as statistically significant.\n",
    "\n",
    "- **Technique**: Calculating Pearson or Spearman correlation coefficients to quantify the strength and direction of the relationship between different variables. Scatter plots will be used for a more nuanced view of these relationships.\n",
    "- **Objective**: To identify which monthly indicators from the high-frequency dataset show a strong and statistically significant correlation with quarterly consumer spending figures. Economic theory will be applied to interpret these correlations, ensuring they align with established economic principles and behaviors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Comparative and Temporal Analysis\n",
    "\n",
    "Undertaken after correlation analysis to delve deeper into the dynamics of the relationships between consumer spending and the identified proxies, providing insights into potential causative or predictive trends.\n",
    "\n",
    "**Lead and Lag Analysis**:\n",
    "\n",
    "- **Technique**: Analysing the time-shifted relationships between consumer spending and the proxies to identify if any indicators consistently lead or lag behind consumer spending patterns.\n",
    "- **Objective**: To discover predictive relationships where certain proxies might signal changes in consumer spending ahead of time or respond with a delay. *While relevant, the Lead and Lag Analysis could become complex and time-consuming. We need to ensure that it directly contributes to the goal of identifying proxies.*\n",
    "\n",
    "**Consumer Behaviour Indicators Correlation**:\n",
    "\n",
    "- **Technique**: Using scatter plots and heatmaps to examine how different indicators relate to consumer spending visually.\n",
    "- **Objective**: To explore more complex relationships between consumer spending and various high-frequency proxies and to identify patterns not evident in standard correlation analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Proxy Evaluation and Variable Selection\n",
    "\n",
    "Essential for finalising the selection of proxies, ensuring they are representative of consumer spending trends and robust under different conditions.\n",
    "\n",
    "**Variable Selection and Reduction**:\n",
    "\n",
    "- **Technique**: Selecting proxies based on correlation outcomes and economic rationale.\n",
    "- **Objective**: To focus on a select group of high-frequency proxies that most accurately reflect and predict trends in consumer spending.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Regression Analysis and Uncertainty Assessment\n",
    "\n",
    "Performed as a concluding analytical step to provide a more nuanced understanding of how each identified proxy affects consumer spending. This step helps quantify the relationships discovered in earlier analyses.\n",
    "\n",
    "**Model Evaluation and Uncertainty Assessment**:\n",
    "\n",
    "- **Technique**: Utilizing advanced statistical techniques, such as bootstrapping or Monte Carlo simulations, to evaluate the robustness of the selected proxies.\n",
    "- **Objective**: To assess the reliability and stability of the chosen proxies under various economic scenarios and conditions. *Techniques like bootstrapping or Monte Carlo simulations might be more advanced than required for this project as the primary aim is to identify proxies rather than to build a predictive model.*\n",
    "\n",
    "**Regression Analysis**\n",
    "\n",
    "- **Technique**: Conduct linear regression analysis to quantify the impact of each selected proxy on consumer spending and assess the significance of regression coefficients.\n",
    "- **Objective**: To determine the strength and nature of the influence that each proxy has on consumer spending, thereby providing a quantitative measure of their relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda7e54fe21129b67f77862937907ee926f057597a3e2fa1e18ac955e40912b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
