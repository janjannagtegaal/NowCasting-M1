{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "# Nowcasting Consumer Expenditure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Uncovering Reliable Proxies for Consumer Spending Behaviour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1. Introduction: The Problem (Why)**\n",
    "\n",
    "The current quarterly GDP reports lag in reflecting the dynamic changes in the economy, impacting decision-makers who rely on timely economic data. This project is devised to mitigate this issue by identifying high-frequency, readily updated data proxies that offer quicker insights into consumer expenditure patterns.\n",
    "\n",
    "**1.2. Project Scope and Objectives (What)**\n",
    "\n",
    "The project's primary objective is to systematically identify, harmonise, and validate high-frequency data sources as proxies for real-time tracking of consumer expenditure in the United States. The goal is to refine these proxies to provide more immediate data on consumer spending habits, thus bridging the gap caused by the delayed reporting of official GDP figures.\n",
    "\n",
    "**Key Questions:**\n",
    "\n",
    "- Which high-frequency data sources can serve as accurate proxies for consumer spending?\n",
    "- How can we validate these proxies against established measures of consumer expenditure?\n",
    "- What techniques can we employ to ensure these proxies offer immediate and reliable insights into current consumer spending trends?\n",
    "- How will we address potential discrepancies between different data sources in terms of scale, units, or reporting standards?\n",
    "- Are there any unforeseen challenges in harmonizing data frequencies (monthly vs. quarterly) that could impact the accuracy of our analysis?\n",
    "- How can we ensure the economic relevance of our findings, beyond statistical correlations?\n",
    "- What contingency plans do we have for dealing with data anomalies or irregularities that might skew our analysis?\n",
    "\n",
    "**1.3. Methodology**\n",
    "\n",
    "The methodology is designed to focus on data preparation and validation:\n",
    "\n",
    "- **Exploratory Data Analysis (EDA)**: To understand the characteristics and quality of the high-frequency monthly indicators and their initial relationships to consumer spending.\n",
    "- **Data Harmonization**: To transform and align the monthly indicators with the quarterly GDP data, using log transformations and adjustments for seasonality and rate of change.\n",
    "- **Proxy Validation**: To establish a correlation with established measures of consumer spending through statistical analysis, ensuring that the proxies are reliable and relevant.\n",
    "\n",
    "**1.4. Assumptions**\n",
    "\n",
    "**Data Quality and Relevance:** We operate under the assumption that the high-frequency data from FRED and other sources accurately reflect current economic trends and consumer sentiments. However, there is an inherent risk of data bias or inaccuracy, which could impact the reliability of our findings.\n",
    "\n",
    "**Predictive Power and Relevance:** While we aim to identify effective proxies for consumer expenditure, there's a risk that these proxies may not fully capture the complexities of consumer behaviour or may not adapt swiftly to sudden economic shifts.\n",
    "\n",
    "**External Factors:** The project also assumes a stable economic environment. Sudden external shocks (like global events or policy changes) could significantly affect consumer behaviour, potentially reducing the predictive accuracy of our proxies.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Primary Dataset Description\n",
    "\n",
    "**Short Description:** The primary dataset is \"Table 1.1.5. Gross Domestic Product\" from the U.S. Bureau of Economic Analysis. It comprises seasonally adjusted quarterly U.S. Gross Domestic Product (GDP) rates in billions of dollars.\n",
    "\n",
    "**Relevance:** The dataset's detailed information on U.S. GDP over several years is integral to the project's goal of nowcasting consumption. The data's granularity and time-series nature will allow for comprehensive analysis and identification of trends, making it pivotal for the project's success.\n",
    "\n",
    "**Data frequency:** The data reflecting the economic output of the United States is crucial for analyzing economic trends and growth patterns. The presentation of data is done quarterly by the GDP component.\n",
    "\n",
    "**Location:** Available at [U.S. Bureau of Economic Analysis](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==). ([BEA](https://apps.bea.gov/iTable/?reqid=19&step=2&isuri=1&categories=survey&_gl=1*j1lvlb*_ga*MTk0MDMyMjk0MC4xNzA1NDk1NTk4*_ga_J4698JNNFT*MTcwNTQ5NTU5OC4xLjEuMTcwNTQ5NzA2MC42MC4wLjA.#eyJhcHBpZCI6MTksInN0ZXBzIjpbMSwyLDMsM10sImRhdGEiOltbImNhdGVnb3JpZXMiLCJTdXJ2ZXkiXSxbIk5JUEFfVGFibGVfTGlzdCIsIjUiXSxbIkZpcnN0X1llYXIiLCIxOTQ3Il0sWyJMYXN0X1llYXIiLCIyMDIzIl0sWyJTY2FsZSIsIi05Il0sWyJTZXJpZXMiLCJRIl1dfQ==))\n",
    "\n",
    "**Format:** CSV\n",
    "\n",
    "**Access Method:** The dataset is readily available and can be easily accessed and downloaded directly from the U.S. Bureau of Economic Analysis website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Secondary Datasets\n",
    "\n",
    "##### Federal Reserve Economic Data (FRED)\n",
    "\n",
    "**Short Description:** This dataset is sourced from the Federal Reserve Bank of St. Louis's FRED macroeconomic database. It contains a variety of economic data points available at monthly intervals, with a particular focus on US GDP data. The data covers consumer spending indicators, a crucial component of the Gross Domestic Product (GDP).\n",
    "\n",
    "**Relevance**: Complements the primary dataset with additional economic indicators, useful for cross-referencing and correlation analysis.\n",
    "\n",
    "**Data frequency:** The monthly frequency of this dataset provides a more detailed temporal resolution than the primary dataset, which may reveal more immediate economic trends. This granularity will be useful in identifying more immediate proxies for nowcasting.\n",
    "\n",
    "**Estimated Size**: 0.6MB\n",
    "\n",
    "**Location**: https://research.stlouisfed.org/econ/mccracken/fred-databases/\n",
    "\n",
    "**Format**: CSV.\n",
    "\n",
    "**Access Method**: Direct download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Setup Environment and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the virtual environment by running in terminal: \n",
    "# python -m venv myenv\n",
    "# source myenv/bin/activate\n",
    "# ! source /myenv/bin/activate\n",
    "\n",
    "# ------- PIP INSTALLS -------\n",
    "# ! pip install --upgrade pip\n",
    "# ! pip install -r requirements.txt\n",
    "\n",
    "# Run the imports file\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbformat\n",
    "# import os\n",
    "\n",
    "# def extract_imports_from_notebook(notebook_path):\n",
    "#     with open(notebook_path, 'r', encoding='utf-8') as file:\n",
    "#         nb = nbformat.read(file, as_version=4)\n",
    "\n",
    "#     imports = set()\n",
    "#     for cell in nb['cells']:\n",
    "#         if cell['cell_type'] == 'code':\n",
    "#             lines = cell['source'].split('\\n')\n",
    "#             for line in lines:\n",
    "#                 if line.startswith('import ') or ' import ' in line:\n",
    "#                     # Handle direct imports like 'import pandas'\n",
    "#                     if line.startswith('import '):\n",
    "#                         lib = line.split()[1].split('.')[0]\n",
    "#                         imports.add(lib)\n",
    "#                     # Handle from imports like 'from sklearn.linear_model import LinearRegression'\n",
    "#                     elif ' import ' in line:\n",
    "#                         lib = line.split()[1]\n",
    "#                         imports.add(lib)\n",
    "\n",
    "#     return imports\n",
    "\n",
    "# # Path to Jupyter notebook\n",
    "# notebook_path = './M1_NowCasting_Consumer_Exp.ipynb'\n",
    "# imports = extract_imports_from_notebook(notebook_path)\n",
    "\n",
    "# # Print out the unique imports and write to requirements.txt\n",
    "# print(\"Libraries to install:\")\n",
    "# with open('requirements.txt', 'w') as f:\n",
    "#     for lib in sorted(imports):\n",
    "#         print(lib)\n",
    "#         f.write(lib + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Standard Library Imports -------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "import types\n",
    "import warnings\n",
    "import subprocess\n",
    "import base64\n",
    "import math\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "# ------- Third-Party Library Imports -------\n",
    "# Data handling and numerical operations\n",
    "import pandas as pd\n",
    "from pandas import NaT\n",
    "import numpy as np\n",
    "\n",
    "# File handling and environment\n",
    "import pyodbc\n",
    "\n",
    "# Utility and display modules\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the display options\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Load and Pre Process BEAU Quarterly GDP dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;\">\n",
    "\n",
    "##### Loads and preprocesses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loads and preprocesses** the GDP data from a CSV file. Process a DataFrame to create a structured description column.\n",
    "\n",
    "**Handling Missing Values**: Utilize median imputation for missing values, as it's less influenced by outliers and provides a more representative central tendency.\n",
    "\n",
    "**Outliers and Anomalies**: Apply Interquartile Range (IQR) or Z-score analysis to identify and address outliers. This step ensures the integrity of data by minimizing the impact of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_gdp_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the GDP data from a CSV file.\n",
    "    Args:file_path (str): The path to the CSV file containing GDP data.\n",
    "    Returns:pandas.DataFrame: Preprocessed GDP data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data with specified rows to skip and number of rows to read\n",
    "    pce_df = pd.read_csv(file_path, skiprows=3, nrows=28)\n",
    "\n",
    "    # Drop the first column (unnecessary or identifier column)\n",
    "    pce_df.drop(pce_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Rename the first column as 'description'\n",
    "    pce_df.rename(columns={pce_df.columns[0]: 'description'}, inplace=True)\n",
    "\n",
    "    # Remove any characters after (and including) the \".\" in column names\n",
    "    pce_df.columns = pce_df.columns.str.replace(r'\\..*', '', regex=True)\n",
    "\n",
    "    # Concatenate the column names with the first row values, handling NaNs\n",
    "    pce_df.columns = pce_df.columns + \" \" + pce_df.iloc[0].fillna('')\n",
    "\n",
    "    # Drop the first row as it's now part of the column names\n",
    "    pce_df.drop(pce_df.index[0], inplace=True)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    pce_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Correct any trailing space issues in the 'description' column name\n",
    "    pce_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    return pce_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_description(pce_df):\n",
    "    \"\"\"\n",
    "    Process a DataFrame to create a structured description column.\n",
    "    This function takes a DataFrame with a 'description' column and adds structure to it\n",
    "    based on indentation levels, indicating hierarchical relationships.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to determine the indentation level (number of leading spaces)\n",
    "    def indentation_level(s):\n",
    "        \"\"\"Return the number of leading spaces in a string, indicating the indentation level.\"\"\"\n",
    "        return len(s) - len(s.lstrip())\n",
    "\n",
    "    # Apply the function to find indentation levels\n",
    "    pce_df['indentation'] = pce_df['description'].apply(indentation_level)\n",
    "\n",
    "    # Initialize an empty list to store the new structured names\n",
    "    structured_names = []\n",
    "    current_parent = \"\"\n",
    "    current_subparent = \"\"\n",
    "\n",
    "    # Iterate through the DataFrame to construct the hierarchical names\n",
    "    for index, row in pce_df.iterrows():\n",
    "        if row['indentation'] == 0:\n",
    "            name = row['description'].strip()\n",
    "            current_parent = name\n",
    "        elif row['indentation'] == 4:\n",
    "            name = f\"{current_parent} : {row['description'].strip()}\"\n",
    "            current_subparent = row['description'].strip()\n",
    "        elif row['indentation'] == 8:\n",
    "            name = f\"{current_parent} : {current_subparent} : {row['description'].strip()}\"\n",
    "        else:\n",
    "            name = row['description'].strip()\n",
    "\n",
    "        structured_names.append(name)\n",
    "\n",
    "    # Assigning the structured names to the 'description' column\n",
    "    pce_df['description'] = structured_names\n",
    "\n",
    "    # Dropping the 'indentation' column as it's no longer needed\n",
    "    pce_df.drop('indentation', axis=1, inplace=True)\n",
    "\n",
    "    return pce_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_short_description(pce_df):\n",
    "    \"\"\"\n",
    "    Create a column 'short_description' in the gdp_df DataFrame with abbreviated descriptions.\n",
    "    Parameters:gdp_df (DataFrame): A DataFrame containing GDP data with a column 'description'.\n",
    "    Returns:DataFrame: The modified DataFrame including a new 'short_description' column.\n",
    "    \"\"\"\n",
    "    def abbreviate_description(desc):\n",
    "\n",
    "        # Define a mapping from full descriptions to their abbreviations\n",
    "        abbreviations = {\n",
    "            \"Gross domestic product\": \"GDP\",\n",
    "            \"Personal consumption expenditures\": \"PCE\",\n",
    "            \"Gross private domestic investment\": \"GPDI\",\n",
    "            \"Net exports of goods and services\": \"NXGS\",\n",
    "            \"Government consumption expenditures and gross investment\": \"GCEGI\",\n",
    "        }\n",
    "\n",
    "        # Split the description into parts and abbreviate each part\n",
    "        parts = desc.split(\" : \")\n",
    "        abbreviated_parts = [abbreviations.get(part, part) for part in parts]\n",
    "\n",
    "        # Join the abbreviated parts and replace spaces with underscores\n",
    "        abrev_descr = \"_\".join(abbreviated_parts).replace(' ', '_')\n",
    "        \n",
    "        # remove all leading \"_\" characters\n",
    "        abrev_descr = abrev_descr.lstrip('_')\n",
    "        \n",
    "        #remove leading and trailing spaces from the description column\n",
    "        abrev_descr = abrev_descr.strip()\n",
    "        \n",
    "        return abrev_descr\n",
    "\n",
    "    # Apply the abbreviation function to each description\n",
    "    pce_df['short_description'] = pce_df['description'].apply(abbreviate_description)\n",
    "    pce_df['description'] = pce_df['description'].str.lstrip(\" :\").str.strip()\n",
    "\n",
    "    # Insert the new column 'short_description' right after the 'description' column\n",
    "    description_index = pce_df.columns.get_loc(\"description\")\n",
    "    pce_df.insert(description_index + 1, 'short_description', pce_df.pop('short_description'))\n",
    "    \n",
    "    #drop the 'description' column\n",
    "    pce_df.drop('description', axis=1, inplace=True)\n",
    "    \n",
    "    #move last row to after 1st row fo readability\n",
    "    last_row = pce_df.iloc[-1].copy()\n",
    "    pce_df = pce_df.iloc[:-1]\n",
    "    pce_df = pd.concat([pce_df.iloc[:1], last_row.to_frame().T, pce_df.iloc[1:]]).reset_index(drop=True)\n",
    "\n",
    "    return pce_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_formats(pce_df):\n",
    "    # Step 1: Extract only non-date columns\n",
    "    non_date_columns = pce_df.columns[:1]  # Assuming first column is non-date column\n",
    "\n",
    "    # Step 2: Extract and transform date columns\n",
    "    date_columns = pce_df.columns[1:]  # Date columns start from the 2nd column\n",
    "\n",
    "    # Function to convert quarter to last date of the quarter\n",
    "    def quarter_to_date(q):\n",
    "        year, quarter = q.split(' Q')\n",
    "        year = int(year)\n",
    "        if quarter == '1':\n",
    "            return f\"{year}-03-31\"\n",
    "        elif quarter == '2':\n",
    "            return f\"{year}-06-30\"\n",
    "        elif quarter == '3':\n",
    "            return f\"{year}-09-30\"\n",
    "        elif quarter == '4':\n",
    "            return f\"{year}-12-31\"\n",
    "\n",
    "    # Apply this function to each of the date columns\n",
    "    transformed_date_columns = [quarter_to_date(col) for col in date_columns]\n",
    "\n",
    "    # Step 3: Combine the columns back together\n",
    "    pce_df.columns = list(non_date_columns) + transformed_date_columns\n",
    "\n",
    "    # Transpose the dataset for easier manipulation (columns become rows)\n",
    "    pce_df = pce_df.set_index('short_description').transpose()\n",
    "\n",
    "    # Converting the index to datetime\n",
    "    pce_df.index = pd.to_datetime(pce_df.index)\n",
    "\n",
    "    pce_df.index.freq = 'Q'\n",
    "\n",
    "    # Convert all columns to numeric\n",
    "    for col in pce_df.columns:\n",
    "        pce_df[col] = pd.to_numeric(pce_df[col], errors='coerce')\n",
    "\n",
    "    return pce_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/bea/bea_usgdp.csv'\n",
    "pce_df = load_and_preprocess_gdp_data(file_path)\n",
    "\n",
    "pce_df = create_structured_description(pce_df)\n",
    "pce_df = create_short_description(pce_df)\n",
    "\n",
    "# extract only PCE data\n",
    "pce_df = pce_df[pce_df['short_description'].str.contains('PCE')]\n",
    "\n",
    "# Call the function with your DataFrame\n",
    "pce_df = transform_date_formats(pce_df)\n",
    "\n",
    "pce_df.to_csv('bea_pce.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;\">\n",
    "\n",
    "##### Visualy inspect the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected function without the unnecessary to_timestamp() conversion\n",
    "def plot_time_series_with_iqr_and_extended_range_subplot(df, ax, column):\n",
    "    # Use the index as it is already in datetime format\n",
    "    datetime_index = df.index\n",
    "\n",
    "    # Calculate statistics\n",
    "    median = df[column].median()\n",
    "    std = df[column].std()\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = median - 2.698 * std\n",
    "    upper_whisker = median + 2.698 * std\n",
    "    \n",
    "    # Plot the time series line graph\n",
    "    ax.plot(datetime_index, df[column], marker='o', markersize=3, color='black', linewidth=1, label='PCE')\n",
    "\n",
    "    # Shade the IQR\n",
    "    ax.fill_between(datetime_index, Q1, Q3, color='grey', alpha=0.4, label='IQR')\n",
    "    \n",
    "    # Shade the extended range\n",
    "    ax.fill_between(datetime_index, lower_whisker, upper_whisker, color='lightgrey', alpha=0.3, label='Extended Range')\n",
    "    \n",
    "    # Mark potential outliers\n",
    "    outliers = df[column][(df[column] < lower_whisker) | (df[column] > upper_whisker)]\n",
    "    ax.scatter(outliers.index, outliers, color='red', zorder=5, label='Outliers')\n",
    "\n",
    "    # Add median line\n",
    "    ax.axhline(median, color='darkgreen', linestyle='--', linewidth=1.0, label='Median')\n",
    "    \n",
    "    # Add upper and lower whiskers lines\n",
    "    ax.axhline(upper_whisker, color='grey', linestyle='--', linewidth=1, label='Upper Whisker')\n",
    "    ax.axhline(lower_whisker, color='grey', linestyle='--', linewidth=1, label='Lower Whisker')\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(column)\n",
    "    ax.set_title(f'{column}')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(False)\n",
    "    \n",
    "def plot_stacked_area_chart(data, ax):\n",
    "    # Plotting the stacked area chart for specified columns\n",
    "    ax.stackplot(data.index, data['PCE_Goods_Durable_goods'], data['PCE_Goods_Nondurable_goods'], data['PCE_Services'], \n",
    "                 labels=['PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services'], \n",
    "                 alpha=0.5)\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Expenditure')\n",
    "    ax.set_title('Stacked Area Chart of PCE Components')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate of change is typically calculated as \n",
    "(\n",
    "Current Value\n",
    "−\n",
    "Previous Value\n",
    "Previous Value\n",
    ")\n",
    "×\n",
    "100\n",
    "%\n",
    "( \n",
    "Previous Value\n",
    "Current Value−Previous Value\n",
    "​\t\n",
    " )×100%, which can be easily computed using the pct_change() function in pandas, and then multiplying by 100 to convert it to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rate of change for each column\n",
    "pce_real_growth = pce_df.pct_change().dropna() * 100\n",
    "\n",
    "# Selecting the relevant columns for correlation\n",
    "columns_for_correlation = ['PCE', 'PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services']\n",
    "correlation_data = pce_df[columns_for_correlation]\n",
    "\n",
    "# Plot stacked area chart and time series with IQR and extended range\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.tight_layout(pad=4.0)\n",
    "plot_stacked_area_chart(pce_df, axs[0])\n",
    "plot_time_series_with_iqr_and_extended_range_subplot(pce_real_growth, axs[1], 'PCE')\n",
    "axs[1].set_title('Rate of Change for PCE')\n",
    "plt.show()\n",
    "\n",
    "# Plot scatter plots with regression lines\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "components = ['PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services']\n",
    "for i, component in enumerate(components):\n",
    "    sns.regplot(x=component, y='PCE', data=correlation_data, ax=axs[i])\n",
    "    axs[i].set_title(f'Scatter Plot of {component} vs PCE')\n",
    "    axs[i].set_xlabel(component)\n",
    "    axs[i].set_ylabel('PCE')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of Scatter Plots with Regression Lines:**\n",
    "\n",
    "Scatter plots with regression lines for each component ('PCE_Goods_Durable_goods', 'PCE_Goods_Nondurable_goods', 'PCE_Services') against 'PCE' illustrate the linear relationships between these components and the total PCE. The regression lines provide a visual indicator of the direction, strength, and linearity of these relationships.\n",
    "\n",
    "**Interpretation of the Correlation Matrix:**\n",
    "\n",
    "The correlation matrix, particularly its visualization through the heatmap, shows the Pearson correlation coefficients between 'PCE' and its components. The coefficients near 1 indicate a very strong positive linear relationship. Specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "#### Remove Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- To carry out this outlier removal, we've created a function named `pce_remove_outliers`. \n",
    "- This function identifies extreme values and replaces them with NaN (missing values). It also keeps track of the year in which each extreme value was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pce_remove_outliers(data):\n",
    "    \"\"\"\n",
    "    Replaces outliers in a DataFrame with NaN based on IQR.\n",
    "    \"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the mask using the typical IQR criterion\n",
    "    mask = (data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))\n",
    "\n",
    "    # Replace extreme values with NaN and store the corresponding values\n",
    "    extreme_values = {}\n",
    "    for column in data.columns:\n",
    "        extreme_values[column] = data[column][mask[column]].dropna().reset_index()\n",
    "\n",
    "    data[mask] = np.nan\n",
    "    return data, extreme_values\n",
    "\n",
    "# Call the function\n",
    "fred_orig, extreme_values = pce_remove_outliers(pce_df)\n",
    "\n",
    "# Print the column names and values where outliers were found\n",
    "for column, values_df in extreme_values.items():\n",
    "    if not values_df.empty:\n",
    "        print(f\"Extreme values for {column}:\")\n",
    "        print(values_df)\n",
    "    else:\n",
    "        print(f\"No extreme values for {column}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pce_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Load and Pre Process FRED monthly  dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;\">\n",
    "\n",
    "##### Loading the FRED data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_fredmd_data` function, below, performs the following actions, once for the FRED-MD dataset and once for the FRED-QD dataset:\n",
    "\n",
    "1. Based on the `vintage` argument, it downloads a particular vintage of these datasets from the base URL https://files.stlouisfed.org/files/htdocs/fred-md into the `orig_m` variable.\n",
    "2. Extracts the column describing which transformation to apply, shortname and description mappings\n",
    "3. Extracts the observation date (from the \"sasdate\" column) and uses it as the index of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fredmd_data(vintage):\n",
    "    \"\"\"\n",
    "    Loads and processes the FRED-MD data.\n",
    "    \"\"\"\n",
    "    # Define the base URL for the FRED-MD dataset\n",
    "    base_url = 'https://files.stlouisfed.org/files/htdocs/fred-md'\n",
    "\n",
    "    # Load the dataset for the specified 'vintage', dropping rows that are entirely NA\n",
    "    fred_orig = pd.read_csv(f'{base_url}/monthly/{vintage}.csv').dropna(how='all')\n",
    "\n",
    "    # Extract transformation codes (second column onwards) from the first row\n",
    "    transform_info = fred_orig.iloc[0, 1:]\n",
    "\n",
    "    # Drop the first row (containing transformation info) from the dataset\n",
    "    fred_orig = fred_orig.iloc[1:]\n",
    "\n",
    "    # Convert 'sasdate' column to a PeriodIndex with monthly frequency for time-series analysis\n",
    "    fred_orig.index = pd.PeriodIndex(fred_orig.sasdate.tolist(), freq='M')\n",
    "\n",
    "    # Remove the 'sasdate' column as it's now set as the index\n",
    "    fred_orig.drop('sasdate', axis=1, inplace=True)\n",
    "\n",
    "    # Return the processed data and the transformation information\n",
    "    return fred_orig, transform_info\n",
    "\n",
    "# Load data for the current vintage and unpack into original data and transformation info\n",
    "fred_orig, transform_info = load_fredmd_data(\"current\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_orig.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0;padding: 5px;\">\n",
    "\n",
    "##### Mapping FRED indices to Economic Data groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import and organize the definitions of economic variables. \n",
    "These definitions are loaded from CSV files corresponding to the FRED-MD and FRED-QD databases. \n",
    "This process ensures that we have a clear and concise understanding of each economic variable in our dataset, which is essential for accurate analysis and interpretation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Column Name Mapping\n",
    "def map_column_names(data, defn_file):\n",
    "    \"\"\"\n",
    "    Maps FRED-MD column names to their descriptions.\n",
    "    \"\"\"\n",
    "    # Load the definitions file, ignoring encoding errors\n",
    "    defn = pd.read_csv(defn_file, encoding_errors='ignore')\n",
    "\n",
    "    # Set the 'fred' column as the index of the definitions DataFrame\n",
    "    defn.index = defn.fred\n",
    "\n",
    "    # Filter the definitions to include only those variables present in the data columns\n",
    "    defn = defn.loc[data.columns.intersection(defn.fred), :]\n",
    "\n",
    "    # Create a dictionary mapping FRED-MD variable names to their descriptions\n",
    "    map_dict = defn['description'].to_dict()\n",
    "\n",
    "    # Replace the names of columns in the dataset with the descriptions from the map\n",
    "    return data[map_dict.keys()].rename(columns=map_dict),defn\n",
    "\n",
    "# Map column names for fred_original \n",
    "column_defn_file = './data/FRED/FRED_Definitions_Mapping/fredmd_definitions.csv'\n",
    "fred_orig,defn = map_column_names(fred_orig, column_defn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_orig.head(2)\n",
    "# defn[[\"description\",\"group\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we get the groups for each series from the definition files above, and then show how many of the series that we'll be using fall into each of the groups.\n",
    "\n",
    "We'll also re-order the series by group, to make it easier to interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping of variable id to group name, for monthly variables\n",
    "groups = defn[['description', 'group']].copy()\n",
    "\n",
    "# Display the number of variables in each group\n",
    "(groups.groupby('group', sort=False)\n",
    "       .count()\n",
    "       .rename({'description': '# series in group'}, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Remove Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Outliers are defined as observations that deviate significantly from the series mean, specifically those that are more than 10 times the interquartile range (IQR) away from the mean.\n",
    "- To carry out this outlier removal, we've created a function named `remove_outliers`. This function identifies extreme values and replaces them with NaN (missing values). It also keeps track of the year in which each extreme value was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    \"\"\"\n",
    "    Replaces outliers in a DataFrame with NaN based on IQR.\n",
    "    \"\"\"\n",
    "    mean = data.mean()\n",
    "    iqr = data.quantile([0.25, 0.75]).diff().T.iloc[:, 1]\n",
    "\n",
    "    # Create a mask to identify extreme values\n",
    "    mask = np.abs(data) > mean + 10 * iqr\n",
    "\n",
    "    # Replace extreme values with NaN and store the corresponding year\n",
    "    extreme_values = {}\n",
    "    for column in data.columns:\n",
    "        extreme_values[column] = data[column][mask[column]].dropna().reset_index()\n",
    "\n",
    "    data[mask] = np.nan\n",
    "    return data, extreme_values\n",
    "\n",
    "# Call the function\n",
    "fred_orig, extreme_values = remove_outliers(fred_orig) # Remove outliers for a specific period\n",
    "\n",
    "# Print the column names and values where outliers were found\n",
    "for column, values_df in extreme_values.items():\n",
    "    if not values_df.empty:\n",
    "        pprint(f\"Extreme values for {column}:\")\n",
    "        pprint(values_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_orig.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "fred_orig.to_csv(\"./results/monthly.csv\")\n",
    "pce_df.to_csv(\"./results/pce.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Data Harmonization and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Standardizing Data by Rate of Change: \n",
    "\n",
    "</div>\n",
    "\n",
    "Objective: To compare different economic indicators on a common scale.\n",
    "Method: We normalize the growth rates of various indicators. This standardization facilitates more meaningful analysis across diverse data points, as it accounts for differences in magnitude and unit measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only observations from the year 2000 onwards\n",
    "fred_orig = fred_orig[fred_orig.index.year >= 2000]\n",
    "pce_df = pce_df[pce_df.index.year >= 2000]\n",
    "\n",
    "# Calculate the month-over-month rate of change for the FRED dataset\n",
    "fred_rate_of_change = fred_orig.pct_change()\n",
    "\n",
    "# Calculate the quarter-over-quarter rate of change for the PCE dataset\n",
    "pce_rate_of_change = pce_df.pct_change()\n",
    "\n",
    "#suggestion\n",
    "# Q-o-Q instead mom, mom can cause seasonality issues, y-o-y is better\n",
    "\n",
    "#harmonise billion features, \n",
    "\n",
    "#rate of change for diff formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Frequency Alignment: \n",
    "</div>\n",
    "- Transform the monthly economic indices from FRED to a quarterly format to align with the BEA’s quarterly GDP data. Calculate the sum or average (as appropriate) of monthly values within each quarter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateTimeIndex to PeriodIndex with quarterly frequency\n",
    "pce_rate_of_change.index = pce_rate_of_change.index.to_period('Q')\n",
    "\n",
    "# Convert the monthly rate of change data to quarterly\n",
    "# We'll use the the sum aggregation method, which is suitable for rate of change data\n",
    "fred_quarterly_rate_of_change = fred_rate_of_change.resample('Q').sum()\n",
    "\n",
    "# print the first 5 rows of the index of both datasets to compare\n",
    "print(fred_quarterly_rate_of_change.index[:5])\n",
    "print(pce_rate_of_change.index[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#DC143C\">\n",
    "\n",
    "##### Possible Transformation of Growth Rates: \n",
    "\n",
    "**Status:** *In Progress* - We are currently in the process of evaluating each indicator to determine the necessity and suitability of log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#DC143C\">\n",
    "\n",
    "**Seasonal Adjustments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust high-frequency data for seasonality, if necessary, to isolate the core economic trends from regular seasonal patterns. This step will make the data more representative of general economic behaviours, irrespective of seasonal influences.\n",
    "\n",
    "- **Technique**: Applying time-series decomposition methods to separate the data into trend, seasonal, and residual components and then adjusting for these seasonal effects.\n",
    "- **Objective**: To accurately capture the underlying trends in consumer spending by removing repetitive seasonal patterns, which are regular but not necessarily related to the economic indicators of interest. *While crucial, we have to ensure this doesn't lead to an overly complex focus on time-series analysis techniques unless they are directly relevant to identifying proxies.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be Conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"color:#DC143C\">\n",
    "\n",
    "**Log Transformation for Monthly Data**\n",
    "\n",
    "</div>\n",
    "\n",
    "- **Rationale:** Logarithmic transformation is used to stabilize the variance in data that exhibits exponential growth or large fluctuations. This is especially crucial for datasets like FRED's, where certain indicators can show significant variability over time.\n",
    "- **Procedure:**\n",
    "  - **Inspect Data:** We start by visually inspecting the PCE rate of change data and the FRED rate of change data for skewness and variance.\n",
    "  - **Apply Transformations:** For indicators showing signs of exponential growth or high variability, we implement logarithmic transformations. This approach helps in linearizing the growth trends and stabilizing the variance.\n",
    "  - **FRED Logarithmic Key Mapping:** FRED provides a logarithmic key mapping for its indicators, which we utilize to ensure consistent and accurate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get transformation codes from fred_defintions\n",
    "# transform_info = transform_info.to_frame().T\n",
    "# transform_info = map_column_names(transform_info, column_defn_file)\n",
    "# transform_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_distribution_stats(df):\n",
    "#     # Calculate skewness, kurtosis, and variance for each column\n",
    "    \n",
    "#     stats_df = pd.DataFrame(index=df.columns, \n",
    "#                             columns=['Skewness', 'Kurtosis', 'Variance', \n",
    "#                                      'Interpretation', 'Transformation', 'Visualization'])\n",
    "\n",
    "#     for column in df.columns:\n",
    "#         stats_df.at[column, 'Skewness'] = df[column].skew()\n",
    "#         stats_df.at[column, 'Kurtosis'] = df[column].kurtosis()\n",
    "#         stats_df.at[column, 'Variance'] = df[column].var()\n",
    "\n",
    "#         # Interpretation of skewness and kurtosis\n",
    "#         skew = stats_df.at[column, 'Skewness']\n",
    "#         kurt = stats_df.at[column, 'Kurtosis']\n",
    "#         transformation = \"None\"\n",
    "#         log_transformation = \"None\"\n",
    "#         visualization = \"Histogram or Boxplot\"\n",
    "\n",
    "#         if np.abs(skew) < 0.5:\n",
    "#             interpretation = 'Fairly Symmetrical'\n",
    "#             if df[column].min() >= 0:\n",
    "#                 transformation = 'Log'\n",
    "#         elif skew >= 0.5:\n",
    "#             interpretation = 'Right Skewed'\n",
    "#             transformation = 'Square Root or Log'\n",
    "#         elif skew <= -0.5:\n",
    "#             interpretation = 'Left Skewed'\n",
    "#             transformation = 'Square or Cube'\n",
    "\n",
    "#         if kurt > 3:\n",
    "#             interpretation += \", Heavy Tails\"\n",
    "#             visualization = \"Boxplot for Outliers\"\n",
    "\n",
    "#         stats_df.at[column, 'Interpretation'] = interpretation\n",
    "#         stats_df.at[column, 'Transformation'] = transformation\n",
    "#         stats_df.at[column, 'Visualization'] = visualization\n",
    "\n",
    "#     # Sort the DataFrame based on the absolute skewness\n",
    "#     stats_df['Absolute Skewness'] = stats_df['Skewness'].abs()\n",
    "#     sorted_stats = stats_df.sort_values(by='Absolute Skewness', ascending=False)\n",
    "\n",
    "#     return sorted_stats.drop('Absolute Skewness', axis=1)\n",
    "\n",
    "# # Example usage with your DataFrame\n",
    "# distribution_stats = calculate_distribution_stats(fred_quarterly_rate_of_change)\n",
    "# distribution_stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(column, transformation_code):\n",
    "#     \"\"\"\n",
    "#     Applies the specified transformation to a Pandas Series.\n",
    "#     Transformation Codes from FRED suggested Description:\n",
    "#     1. No Transformation, 2. First Difference, 3. Second Difference,\n",
    "#     4. Log Transformation, 5. Log First Difference, 6. Log Second Difference,\n",
    "#     7. Exact Percent Change\n",
    "#     \"\"\"\n",
    "#     # Multiplier for quarterly data; if data is quarterly, multiply by 4, else 1\n",
    "#     mult = 4 if column.index.freqstr[0] == 'Q' else 1\n",
    "\n",
    "#     if transformation_code == 1:\n",
    "#         # No transformation, return the column as is\n",
    "#         return column\n",
    "#     if transformation_code == 2:\n",
    "#         # First Difference: Subtract each element from its predecessor\n",
    "#         # Useful for converting a series to its change values\n",
    "#         return column.diff()\n",
    "#     if transformation_code == 3:\n",
    "#         # Second Difference: Apply first difference twice\n",
    "#         # Useful when first difference is insufficient to achieve stationarity\n",
    "#         return column.diff().diff()\n",
    "#     if transformation_code == 4:\n",
    "#         # Log Transformation: Apply natural logarithm\n",
    "#         # Useful for data with exponential growth patterns\n",
    "#         return np.log(column)\n",
    "#     if transformation_code == 5:\n",
    "#         # Log First Difference: Apply log transformation, then first difference\n",
    "#         # Multiplied by 100 for percentage change, especially useful for financial data\n",
    "#         return np.log(column).diff() * 100 * mult\n",
    "#     if transformation_code == 6:\n",
    "#         # Log Second Difference: Apply log transformation, then second difference\n",
    "#         # Similar to Code 5 but provides a more refined measure of change\n",
    "#         return np.log(column).diff().diff() * 100 * mult\n",
    "#     if transformation_code == 7:\n",
    "#         # Exact Percent Change: Calculate the percentage change from one period to the next\n",
    "#         # Useful for directly understanding growth rates\n",
    "#         return ((column / column.shift(1))**mult - 1.0) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Transformation Code 5 for 'PCE', and Code 6 for the rest\n",
    "# pce_df_transformed = pce_df.apply(lambda col: transform(col, 5 if col.name == 'PCE' else 6))\n",
    "# # Apply log transformations using original column names according to FRED guidelines\n",
    "# # fred_log_transform = fred_quarterly_rate_of_change.apply(lambda col: transform(col, transform_info[col.name][0]))\n",
    "# pce_df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty DataFrame to store the transformed data\n",
    "# fred_log_transform = pd.DataFrame(index=fred_quarterly_rate_of_change.index)\n",
    "\n",
    "# # Iterate through each column in fred_quarterly_rate_of_change\n",
    "# for column_name in fred_quarterly_rate_of_change.columns:\n",
    "#     # Fetch the transformation code for the current column from transform_info\n",
    "#     transformation_code = transform_info.at[0, column_name]\n",
    "\n",
    "#     # Apply the transformation using the transform function\n",
    "#     transformed_column = transform(fred_quarterly_rate_of_change[column_name], transformation_code)\n",
    "\n",
    "#     # Store the transformed column in the new DataFrame\n",
    "#     fred_log_transform[column_name] = transformed_column\n",
    "\n",
    "# fred_log_transform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Quarterly Data Integration\n",
    "\n",
    "</div>\n",
    "- We will merge quarterly BEA PCE rate of change data framework with the FRED transformed quarter-over-quarter rate of change into a unified framework using pandas, ensuring seamless integration and compatibility. \n",
    "- This step is vital for consolidating different economic indicators into a single, comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pce_rate_of_change.to_csv(\"./results/pce_rate_of_change.csv\")\n",
    "# fred_quarterly_rate_of_change.to_csv(\"./results/fred_quarterly_rate_of_change.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep \"PCE\" column\n",
    "pce_rate_of_change = pce_rate_of_change[['PCE']]\n",
    "\n",
    "# Joining the datasets on their index values\n",
    "joined_dataset = pce_rate_of_change.join(fred_quarterly_rate_of_change, how='outer')\n",
    "\n",
    "# Calculate the Pearson correlation matrix for the joined dataset\n",
    "correlation_matrix = joined_dataset.corr()\n",
    "\n",
    "# Extract the correlations related to 'PCE'\n",
    "pce_correlations = correlation_matrix['PCE'].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations related to 'PCE'\n",
    "pce_correlations.head(20)  # Displaying the top few correlations for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#00BFFF\">\n",
    "\n",
    "## Linear Regression Analysis to Determine Variable Influence on PCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook conducts a linear regression analysis to explore how different variables in our dataset influence the Private Consumption Expenditure (PCE). The primary goal is to determine the strength of the linear relationship each variable has with PCE, quantified using the \\( R^2 \\) (coefficient of determination) metric.Variables with higher \\( R^2 \\) values are of particular interest as they may be key drivers of PCE variations.By understanding these relationships, we can better comprehend the dynamics influencing Private Consumption Expenditure, an essential component of economic analysis.\n",
    "\n",
    "**Steps in the Analysis:**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - We exclude 'PCE' from the list of independent variables as it is our dependent variable.\n",
    "   - The dataset is cleaned to remove any rows with NaN or infinite values to ensure the validity of our regression analysis.\n",
    "\n",
    "2. **Linear Regression:**\n",
    "   - For each independent variable, we perform a simple linear regression against 'PCE'.\n",
    "   - Only variables with a sufficient number of observations (set by `min_threshold`) are considered to ensure robust regression results.\n",
    "\n",
    "3. **Calculation of \\( R^2 \\) Values:**\n",
    "   - After fitting the model, we predict 'PCE' using each independent variable and compute the \\( R^2 \\) value.\n",
    "   - The \\( R^2 \\) value indicates how much of the variance in 'PCE' is explained by the variable. A higher \\( R^2 \\) suggests a stronger explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Prepare the independent variables (excluding 'PCE')\n",
    "independent_vars = joined_dataset.drop(columns=['PCE']).columns\n",
    "\n",
    "# Prepare the dependent variable 'PCE'\n",
    "dependent_var = joined_dataset['PCE']\n",
    "\n",
    "# Setting a minimum threshold for the number of observations required for regression\n",
    "min_threshold = 30\n",
    "\n",
    "# Dictionary to store R^2 values for each variable\n",
    "r2_values = {}\n",
    "\n",
    "# Perform linear regression for each independent variable\n",
    "for var in independent_vars:\n",
    "    # Drop rows where either the independent or dependent variable is NaN or infinite\n",
    "    combined_data = joined_dataset[[var, 'PCE']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    if len(combined_data) >= min_threshold:\n",
    "        # Prepare the data for regression\n",
    "        X = combined_data[var].values.reshape(-1, 1)\n",
    "        y = combined_data['PCE'].values\n",
    "\n",
    "        # Create and fit the model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Predict and calculate R^2\n",
    "        predictions = model.predict(X)\n",
    "        r2_values[var] = r2_score(y, predictions)\n",
    "\n",
    "# Sorting the R^2 values\n",
    "r2_values_sorted = dict(sorted(r2_values.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Assuming 'defn' is a DataFrame as described\n",
    "groups = defn.set_index('description')['group'].to_dict()\n",
    "\n",
    "# Displaying the top few R^2 values for brevity\n",
    "list(r2_values_sorted.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Grouping, Sorting and Displaying Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - The \\( R^2 \\) values are sorted in descending order, and the top results are displayed.\n",
    "   - This ranking helps us identify which variables have the most significant linear relationship with 'PCE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping the R^2 values to their respective groups\n",
    "grouped_r2_values = {}\n",
    "for var, r2 in r2_values.items():\n",
    "    group = groups.get(var, 'Unknown')  # Default to 'Unknown' if no group is found\n",
    "    if group not in grouped_r2_values:\n",
    "        grouped_r2_values[group] = {}\n",
    "    grouped_r2_values[group][var] = r2\n",
    "    \n",
    "#grouped_r2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sorted DataFrame for each group\n",
    "group_dfs = []\n",
    "for group, indicators in grouped_r2_values.items():\n",
    "    # Create DataFrame and sort by R^2 values\n",
    "    group_df = pd.DataFrame(list(indicators.items()), columns=['Economic Indicator', 'R^2'])\n",
    "    group_df.sort_values(by='R^2', ascending=False, inplace=True)  # Sort by R^2 in descending order\n",
    "    group_df.reset_index(drop=True, inplace=True)  # Reset index to maintain sorted order\n",
    "    group_dfs.append(group_df)\n",
    "\n",
    "# Concatenate all sorted group DataFrames horizontally\n",
    "final_df = pd.concat(group_dfs, axis=1, keys=grouped_r2_values.keys())\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the R^2 values\n",
    "# plt.figure(figsize=(7, 3))\n",
    "# plt.bar(range(len(r2_values_sorted)), list(r2_values_sorted.values()))\n",
    "# plt.title(r'$R^2$ - Regression on Individual Factors', fontsize=14, fontweight=600)\n",
    "# plt.xlabel('Variables')\n",
    "# plt.ylabel(r'$R^2$ Value')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Determine the number of groups\n",
    "num_groups = len(grouped_r2_values)\n",
    "\n",
    "# Create subplots with shared y-axis\n",
    "fig = make_subplots(rows=num_groups, cols=1, shared_yaxes=True, subplot_titles=list(grouped_r2_values.keys()))\n",
    "\n",
    "# Plot each group in a separate subplot with sorted values\n",
    "for i, (group, values) in enumerate(grouped_r2_values.items(), start=1):\n",
    "    # Sort the values in descending order\n",
    "    sorted_values = dict(sorted(values.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=list(sorted_values.keys()), y=list(sorted_values.values()), name=group),\n",
    "        row=i, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000,  # Adjust the height depending on the number of subplots\n",
    "    title_text=r'$R^2$ - Regression on Individual Factors by Group',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update y-axis label and hide x-axis tick labels\n",
    "for i in range(1, num_groups + 1):\n",
    "    fig.update_xaxes(title_text='', showticklabels=False, row=i, col=1)\n",
    "fig.update_yaxes(title_text=r'R^2 Value', row=num_groups, col=1)\n",
    "\n",
    "#update subplot y-axis range to all have value 0-1\n",
    "# for i in range(1, num_groups + 1):\n",
    "#     fig.update_yaxes(range=[0, 1], row=i, col=1)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grouping the R^2 values based on the 'group' they belong to\n",
    "# grouped_r2 = defn.set_index('description').join(pd.Series(r2_values, name='R2')).reset_index()\n",
    "\n",
    "# # Sort the R2 values within each group\n",
    "# grouped_r2_sorted = grouped_r2.groupby('group').apply(lambda x: x.sort_values('R2', ascending=True)).reset_index(drop=True)\n",
    "\n",
    "# # Adjust the code here if 'description' is not a column\n",
    "# # Make sure 'description' is a column in grouped_r2_sorted\n",
    "# if 'description' not in grouped_r2_sorted.columns:\n",
    "#     grouped_r2_sorted['description'] = grouped_r2_sorted.index\n",
    "\n",
    "# # Create lists to hold the sorted variables, their R2 values, and group names for labeling\n",
    "# sorted_vars = []\n",
    "# sorted_r2_values = []\n",
    "# group_labels = []\n",
    "\n",
    "# # For each group, extend the lists with the sorted variables and their R2 values\n",
    "# for group, data in grouped_r2_sorted.groupby('group'):\n",
    "#     sorted_r2_values.extend(data['R2'])  # This will be the y-axis values\n",
    "#     group_labels.extend([group] * len(data))  # This will repeat the group name for each variable\n",
    "\n",
    "# # Create the midpoints for the group labels on the x-axis\n",
    "# midpoints = []\n",
    "# last_index = 0\n",
    "# for group, count in grouped_r2_sorted['group'].value_counts().sort_index().iteritems():\n",
    "#     midpoint = last_index + count / 2\n",
    "#     midpoints.append(midpoint)\n",
    "#     last_index += count\n",
    "\n",
    "# with sns.color_palette('deep'):\n",
    "#     fig, ax = plt.subplots(figsize=(10, 7))  # Adjusted figure size for horizontal layout\n",
    "#     bars = ax.barh(range(len(sorted_r2_values)), sorted_r2_values)\n",
    "#     ax.set_yticks(midpoints)\n",
    "#     ax.set_yticklabels(grouped_r2_sorted['group'].unique(), va='center') # rotation=45\n",
    "\n",
    "#     # Adding shaded bands for groups\n",
    "#     last_index = 0\n",
    "#     for count in grouped_r2_sorted['group'].value_counts().sort_index():\n",
    "#         ax.fill_betweenx(np.arange(last_index, last_index + count), 0, 1.2, color='k', alpha=0.1)\n",
    "#         last_index += count\n",
    "\n",
    "#     ax.set_xlim(0, 1.0)\n",
    "#     ax.set_ylim(-1, len(sorted_r2_values))\n",
    "#     #set x axis label\n",
    "#     ax.set_xlabel(r'R^2 Value')\n",
    "#     fig.tight_layout()\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#5F9EA0\">\n",
    "\n",
    "##### Quality Assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Final anomaly Detection and Correction**: Employing statistical methods to detect and correct anomalies ensures that our analysis is based on accurate and representative data, free from distortions that could lead to erroneous conclusions.\n",
    "\n",
    "**Consistency Checks**: Conducting thorough checks for data consistency, especially when integrating diverse data sources, is essential to validate the reliability and accuracy of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "## Stationarity Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Stationarity Assessment**: Using tests like the Augmented Dickey-Fuller ensures that our time series data is suitable for modelling and forecasting, as many statistical models require stationarity for valid results.\n",
    "\n",
    "**Addressing Non-Stationarity**: Techniques such as differencing or transformation will be applied to achieve stationarity, which is crucial for the accuracy and reliability of our predictive models and correlation analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "</div>\n",
    "\n",
    "Performed early in the project to get an overview of the data's characteristics. This step is crucial for identifying the most relevant variables for analysis, understanding the data's basic structure, and ensuring that hypotheses are grounded in both statistical findings and economic logic.\n",
    "\n",
    "- **Technique**: Using statistical tools to summarise the data, visualising distributions with histograms, identifying correlations with scatter plots, and detecting patterns and outliers with box plots.\n",
    "- **Objective**: To gain an initial understanding of data trends, outliers, and correlations and to identify any anomalies or irregularities that may influence further analysis. Hereafter we will incorporate economic theories to hypothesise potential relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "### Correlation and Proxy Validation Analysis\n",
    "\n",
    "</div>\n",
    "\n",
    "Implemented after seasonality adjustments to ensure that the relationships being analysed and the proxies being identified are not influenced by seasonal fluctuations and confirm that identified relationships are economically plausible as well as statistically significant.\n",
    "\n",
    "- **Technique**: Calculating Pearson or Spearman correlation coefficients to quantify the strength and direction of the relationship between different variables. Scatter plots will be used for a more nuanced view of these relationships.\n",
    "- **Objective**: To identify which monthly indicators from the high-frequency dataset show a strong and statistically significant correlation with quarterly consumer spending figures. Economic theory will be applied to interpret these correlations, ensuring they align with established economic principles and behaviors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "### Comparative and Temporal Analysis\n",
    "\n",
    "</div>\n",
    "\n",
    "Undertaken after correlation analysis to delve deeper into the dynamics of the relationships between consumer spending and the identified proxies, providing insights into potential causative or predictive trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lead and Lag Analysis**:\n",
    "\n",
    "- **Technique**: Analysing the time-shifted relationships between consumer spending and the proxies to identify if any indicators consistently lead or lag behind consumer spending patterns.\n",
    "- **Objective**: To discover predictive relationships where certain proxies might signal changes in consumer spending ahead of time or respond with a delay. *While relevant, the Lead and Lag Analysis could become complex and time-consuming. We need to ensure that it directly contributes to the goal of identifying proxies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consumer Behaviour Indicators Correlation**:\n",
    "\n",
    "- **Technique**: Using scatter plots and heatmaps to examine how different indicators relate to consumer spending visually.\n",
    "- **Objective**: To explore more complex relationships between consumer spending and various high-frequency proxies and to identify patterns not evident in standard correlation analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "### Proxy Evaluation and Variable Selection\n",
    "\n",
    "</div>\n",
    "\n",
    "Essential for finalising the selection of proxies, ensuring they are representative of consumer spending trends and robust under different conditions.\n",
    "\n",
    "**Variable Selection and Reduction**:\n",
    "\n",
    "- **Technique**: Selecting proxies based on correlation outcomes and economic rationale.\n",
    "- **Objective**: To focus on a select group of high-frequency proxies that most accurately reflect and predict trends in consumer spending.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be conducted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:#FFA500\">\n",
    "\n",
    "### Regression Analysis and Uncertainty Assessment\n",
    "\n",
    "</div>\n",
    "\n",
    "Performed as a concluding analytical step to provide a more nuanced understanding of how each identified proxy affects consumer spending. This step helps quantify the relationships discovered in earlier analyses.\n",
    "\n",
    "**Model Evaluation and Uncertainty Assessment**:\n",
    "\n",
    "- **Technique**: Utilizing advanced statistical techniques, such as bootstrapping or Monte Carlo simulations, to evaluate the robustness of the selected proxies.\n",
    "- **Objective**: To assess the reliability and stability of the chosen proxies under various economic scenarios and conditions. *Techniques like bootstrapping or Monte Carlo simulations might be more advanced than required for this project as the primary aim is to identify proxies rather than to build a predictive model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Regression Analysis**\n",
    "\n",
    "- **Technique**: Conduct linear regression analysis to quantify the impact of each selected proxy on consumer spending and assess the significance of regression coefficients.\n",
    "- **Objective**: To determine the strength and nature of the influence that each proxy has on consumer spending, thereby providing a quantitative measure of their relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
